[
  {
    "objectID": "datasets/index.html",
    "href": "datasets/index.html",
    "title": "Datasets",
    "section": "",
    "text": "On ProteomicsML you will find datasets for beginners and experts in the field alike. Download, and explore the intricate nature of mass spectrometry data.\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/index.html#fragmentation",
    "href": "datasets/index.html#fragmentation",
    "title": "Datasets",
    "section": "Fragmentation",
    "text": "Fragmentation\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nNIST Peptide libraries\n\n\nSep 22, 2022\n\n\n\n\nProteomeTools\n\n\nSep 22, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datasets/index.html#ion-mobility",
    "href": "datasets/index.html#ion-mobility",
    "title": "Datasets",
    "section": "Ion mobility",
    "text": "Ion mobility\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nMeier et al.¬†TIMS\n\n\nSep 22, 2022\n\n\n\n\nVan Puyvelde et al.¬†TWIMS\n\n\nSep 22, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datasets/index.html#retention-time",
    "href": "datasets/index.html#retention-time",
    "title": "Datasets",
    "section": "Retention time",
    "text": "Retention time\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nDLOmix\n\n\nSep 22, 2022\n\n\n\n\nProteomeTools\n\n\nSep 22, 2022\n\n\n\n\nSharma et al.¬†HeLa\n\n\nSep 22, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datasets/ionmobility/Meier_TIMS.html",
    "href": "datasets/ionmobility/Meier_TIMS.html",
    "title": "Meier et al.¬†TIMS",
    "section": "",
    "text": "Attributes\n\ndata type: Peptide ion mobility\ntitle: Deep learning the collisional cross sections of the peptide universe from a million experimental values\ntag: Meier_IM_CCS\ndata publication: https://doi.org/10.1074/mcp.tir118.000900\nML publication: https://doi.org/10.1038/s41467-021-21352-8\nsource dataset identifier: PXD010012, PXD019086, PXD017703\nspecies: Homo sapiens (Human), Saccharomyces cerevisiae (Baker‚Äôs yeast)\nsize: 718.917 (large)\nformat: CSV\ncolumns: index, Modified sequence, Charge, Mass, Intensity, Retention time, CCS, PT\nmass modifications: unmodified & oxidation & acetylation & carbamidomethyl\nionmobility_type: TIMS\n\n\n\nData description\nMS raw files were analyzed with MaxQuant version 1.6.5.0, which extracts 4D isotope patterns (‚Äòfeatures‚Äô) and associated MS/MS spectra. The built-in search engine Andromeda74 was used to match observed fragment ions to theoretical peptide fragment ion masses derived from in silico digests of a reference proteome and a list of 245 potential contaminants using the appropriate digestion rules for each proteolytic enzyme (trypsin, LysC or LysN). We allowed a maximum of two missing values and required a minimum sequence length of 7 amino acids while limiting the maximum peptide mass to 4600‚ÄâDa. Carbamidomethylation of cysteine was defined as a fixed modification, and oxidation of methionine and acetylation of protein N-termini were included in the search as variable modifications. Reference proteomes for each organism including isoforms were accessed from UniProt (Homo sapiens: 91,618 entries, 2019/05; E. coli: 4403 entries, 2019/01; C. elegans: 28,403 entries, 2019/01; S. cerevisiae: 6049 entries, 2019/01; D. melanogaster: 23,304 entries, 2019/01). The synthetic peptide library (ProteomeTools54) was searched against the entire human reference proteome. The maximum mass tolerances were set to 20 and 40‚Äâppm for precursor and fragment ions, respectively. False discovery rates were controlled at 1% on both the peptide spectrum match and protein level with a target-decoy approach. The analyses were performed separately for each organism and each set of synthetic peptides (‚Äòproteotypic set‚Äô, ‚ÄòSRM atlas‚Äô, and ‚Äòmissing gene set‚Äô). To demonstrate the utility of CCS prediction, we re-analyzed three diaPASEF experiments from Meier et al.55 with Spectronaut 14.7.201007.47784 (Biognosys AG), replacing experimental ion mobility values in the spectral library with our predictions. Singly charged peptide precursors were excluded from this analysis as the neural network was exclusively trained with multiply charged peptides.\n\n\nSample protocol description\nIn bottom-up proteomics, peptides are separated by liquid chromatography with elution peak widths in the range of seconds, while mass spectra are acquired in about 100 microseconds with time-of-fight (TOF) instruments. This allows adding ion mobility as a third dimension of separation. Among several formats, trapped ion mobility spectrometry (TIMS) is attractive due to its small size, low voltage requirements and high efficiency of ion utilization. We have recently demonstrated a scan mode termed parallel accumulation ‚Äì serial fragmentation (PASEF), which multiplies the sequencing speed without any loss in sensitivity (Meier et al., PMID: 26538118). Here we introduce the timsTOF Pro instrument, which optimally implements online PASEF. It features an orthogonal ion path into the ion mobility device, limiting the amount of debris entering the instrument and making it very robust in daily operation. We investigate different precursor selection schemes for shotgun proteomics to optimally allocate in excess of 100 fragmentation events per second. More than 800,000 fragmentation spectra in standard 120 min LC runs are easily achievable, which can be used for near exhaustive precursor selection in complex mixtures or re-sequencing weak precursors. MaxQuant identified more than 6,000 proteins in single run HeLa analyses without matching to a library, and with high quantitative reproducibility (R > 0.97). Online PASEF achieves a remarkable sensitivity with more than 2,000 proteins identified in 30 min runs of only 10 ng HeLa digest. We also show that highly reproducible collisional cross sections can be acquired on a large scale (R > 0.99). PASEF on the timsTOF Pro is a valuable addition to the technological toolbox in proteomics, with a number of unique operating modes that are only beginning to be explored.\n\n\nData analysis protocol\nSee Data description\n\n\nComments\n/\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/ionmobility/index.html",
    "href": "datasets/ionmobility/index.html",
    "title": "Ion mobility",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/ionmobility/VanPuyvelde_TWIMS.html",
    "href": "datasets/ionmobility/VanPuyvelde_TWIMS.html",
    "title": "Van Puyvelde et al.¬†TWIMS",
    "section": "",
    "text": "Attributes\n\ndata type: Peptide ion mobility\ntitle: A comprehensive LFQ benchmark dataset on modern day acquisition strategies in proteomics\ntag: VanPuyvelde_TWIMS_CCS\ndata publication: https://doi.org/10.1038/s41597-022-01216-6\nML publication: -\nsource dataset identifier: PXD028735\nspecies: Homo sapiens (Human), Saccharomyces cerevisiae (Baker‚Äôs yeast), Escherichia coli (E. coli)\nsize: 6.268 (small)\nformat: tsv\ncolumns: Modified sequence, Charge, CCS, Ion Mobility, Ion Mobility Units, High Energy, Ion Mobility Offset\nmass modifications: unmodified & oxidation & acetylation & carbamidomethyl\nionmobility_type: TWIMS\n\n\n\nData description\nFrom the original paper:\nAn M-class LC system (Waters Corporation, Milford, MA) was equipped with a 1.7‚Äâ¬µm CSH 130 C18 300‚Äâ¬µm‚Äâ√ó 100‚Äâmm column, operating at 5‚Äâ¬µL/min with a column temperature of 55‚Äâ¬∞C. Mobile phase A was UPLC-grade water containing 0.1% (v/v) FA and 3% DMSO, mobile phase B was ACN containing 0.1% (v/v) FA. Peptides were separated using a linear gradient of 3‚àí30% mobile phase B over 120‚Äâminutes. All experiments were conducted on a Synapt G2-Si mass spectrometer (Waters Corporation, Wilmslow, UK). The ESI Low Flow probe capillary voltage was 3‚ÄâkV, sampling cone 60‚ÄâV, source offset 60‚ÄâV, source temperature 80‚Äâ¬∞C, desolvation temperature 350‚Äâ¬∞C, cone gas 80‚ÄâL/hr, desolvation gas 350‚ÄâL/hr, and nebulizer pressure 2.5‚Äâbar. A lock mass reference signal of GluFibrinopeptide B (m/z 785.8426) was sampled every 30‚Äâs.\n\n\nSample protocol description\nFrom the original paper:\nMass spectrometry-compatible Human K562 (P/N: V6951) and Yeast (P/N: V7461) protein digest extracts were purchased from Promega (Madison, Wisconsin, United States). Lyophilised MassPrep Escherichia.coli digest standard (P/N:186003196) was purchased from Waters Corporation (Milford, Massachusetts, United States). The extracts were reduced with dithiothreitol (DTT), alkylated with iodoacetamide (IAA) and digested with sequencing grade Trypsin(-Lys C) by the respective manufacturers. The digested protein extracts were reconstituted in a mixture of 0.1% Formic acid (FA) in water (Biosolve B.V, Valkenswaard, The Netherlands) and spiked with iRT peptides (Biognosys, Schlieren, Switzerland) at a ratio of 1:20‚Äâv/v. Two master samples A and B were created similar to Navarro et al., each in triplicate, as shown in Fig. 1. Sample A was prepared by mixing Human, Yeast and E.coli at 65%, 30% and 5% weight for weight (w/w), respectively. Sample B was prepared by mixing Human, Yeast and E.coli protein digests at 65%, 15%, 20% w/w, respectively. The resulting samples have logarithmic fold changes (log2FCs) of 0, ‚àí1 and 2 for respectively Human, Yeast and E.coli. One sixth of each of the triplicate master batches of A and B were mixed to create a QC sample, containing 65% w/w Human, 22.5% w/w Yeast and 12.5% w/w E.coli.\n\n\nData analysis protocol\nSee Data description\n\n\nComments:\n/\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/retentiontime/Sharma_HeLa_RT.html",
    "href": "datasets/retentiontime/Sharma_HeLa_RT.html",
    "title": "Sharma et al.¬†HeLa",
    "section": "",
    "text": "Download\n ### Attributes - data type: Peptide retention time - title: Kirill Pevzner ‚ÄúProteomics Retention Time Prediction‚Äù dataset from Sharma et al.¬†HeLa data from kaggle - tag: Sharma_HeLa_RT - data publication:  - ML publication:  - source dataset identifier:  - species: Homo sapiens (human) - size: 14361 peptides - format: TSV - columns: peptide sequence, uncalibrated elution time - chromatography_column_type: \n\n\nData description\nThis dataset was downloaded from kaggle (https://www.kaggle.com/datasets/kirillpe/proteomics-retention-time-prediction) It is a simple list of peptide sequences and uncalibrated retention times in seconds specific to one dataset. The pedigree of the data is not well known.\n\n\nComments\n\nReference is Sharma et al., but exact publication is unknown\nThe kaggle web page lists a filename ‚Äúmod.txt‚Äù, but the data archive includes only one file ‚Äúunmod.txt‚Äù\nThere are 14,361 data lines in the file (plus 1 header line)\nHeader line is ‚Äúsequence RT‚Äù\nNone of the peptides have a mass modification listed\nPresumably this includes only peptides with no mass modification noted in the id, rather than stripped of mods\nAre any peptide sequences repeated?\nThe false discovery rate in peptide identifications is not currently known (presumably some are wrong ids)\nThis may possibly be the unmodified peptides from PXD000612\nThis may possibly be the unmodified peptides from https://pubmed.ncbi.nlm.nih.gov/25159151/\nThis is a phospho-enriched dataset. Perhaps only the unmodified peptides are offered\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/retentiontime/DLOmix.html",
    "href": "datasets/retentiontime/DLOmix.html",
    "title": "DLOmix",
    "section": "",
    "text": "Download\n   \n\n\nAttributes\n\ndata type: Peptide retention time\ntitle: DLOmix deep learning in proteomics python framework for retention time\ntag: DLOmix_RT\ndata publication: \nML publication: \nsource dataset identifier: \nspecies: Homo sapiens (human)\nsize: 27160 in train, 6800 in validation, 6000 in test, and 27200 in train/val\nformat: CSV\ncolumns: peptide sequence, iRT calibrated retention time (~minutes)\nchromatography_column_type: \nurl: https://pypi.org/project/dlomix/\nurl: https://github.com/wilhelm-lab/dlomix\nurl: https://github.com/wilhelm-lab/dlomix/tree/develop/example_dataset\n\n\n\nData description\nThis dataset is from the DLOmix project, which includes train, validation, and test sets of\n\n\nComments\n\nTutorial \n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/retentiontime/index.html",
    "href": "datasets/retentiontime/index.html",
    "title": "Retention time",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/retentiontime/ProteomeTools_RT.html",
    "href": "datasets/retentiontime/ProteomeTools_RT.html",
    "title": "ProteomeTools",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/retentiontime/ProteomeTools_RT.html#data-description",
    "href": "datasets/retentiontime/ProteomeTools_RT.html#data-description",
    "title": "ProteomeTools",
    "section": "data description",
    "text": "data description\nThe ProteomeTools project aims to derive molecular and digital tools from the human proteome to facilitate biomedical and life science research. Here, we describe the generation and multimodal LC-MS/MS analysis of >350,000 synthetic tryptic peptides representing nearly all canonical human gene products. This resource will be extended to 1.4 million peptides within two years and all data will be made available to the public in ProteomicsDB."
  },
  {
    "objectID": "datasets/retentiontime/ProteomeTools_RT.html#sample-protocol-description",
    "href": "datasets/retentiontime/ProteomeTools_RT.html#sample-protocol-description",
    "title": "ProteomeTools",
    "section": "sample protocol description:",
    "text": "sample protocol description:\nTryptic peptides were individually synthesized by solid phase synthesis, combined into pools of ~1,000 peptides and measured on an Orbitrap Fusion mass spectrometer. For each peptide pool, an inclusion list was generated to target peptides for fragmentation in further LC-MS experiments using five fragmentation methods (HCD, CID, ETD, EThCD, ETciD) with ion trap or Orbitrap readout and HCD spectra were recorded at 6 different collision energies."
  },
  {
    "objectID": "datasets/retentiontime/ProteomeTools_RT.html#data-anaylsis-protocol",
    "href": "datasets/retentiontime/ProteomeTools_RT.html#data-anaylsis-protocol",
    "title": "ProteomeTools",
    "section": "data anaylsis protocol:",
    "text": "data anaylsis protocol:\nLC-MS runs were individually analyzed using MaxQuant 1.5.3.30.\n\nComments\n\nThe full dataset was reduced in size to small, medium, and large sizes\nLink to FigShare area?"
  },
  {
    "objectID": "datasets/fragmentation/NIST.html",
    "href": "datasets/fragmentation/NIST.html",
    "title": "NIST Peptide libraries",
    "section": "",
    "text": "Downloads\n\n\n\n\n\n\n\nAttributes\n\ndata type: Fragmentation intensity\ntitle: NIST\ntag: NIST\ndata publication: https://chemdata.nist.gov/dokuwiki/lib/exe/fetch.php?media=peptidew:sergey_sheetlin_asms2020.pdf\nML publication: /\nsource dataset identifier: /\nspecies: Homo sapiens (human)\nsize: 646 MB (gzipped)\nformat: MSP\ncolumns: /\nmass modifications: unmodified & oxidation\nchromatography_column_type: multiple\n\n\n\nData description\nConsensus spectral libraries generated by NIST, the US National Institute of Standards and Technology.\n\n\nComments\n\nData on chemdata.nist.gov\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/fragmentation/index.html",
    "href": "datasets/fragmentation/index.html",
    "title": "Fragmentation",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/fragmentation/ProteomeTools_FI.html",
    "href": "datasets/fragmentation/ProteomeTools_FI.html",
    "title": "ProteomeTools",
    "section": "",
    "text": "Downloads\n \n\n\nAttributes\n\ndata type: Fragmentation intensity\ntitle: ProteomeTools synthetic peptides\ntag: ProteomeTools\ndata publication: https://doi.org/10.1038/nmeth.4153\nML publication: https://doi.org/10.1038/s41592-019-0426-7\nsource dataset identifier: PXD004732\nspecies: Homo sapiens (human)\nsize: Subset: Train (4.87 GB). Holdout (250 MB). Non-Tryptic:\n\nformat: hdf5\ncolumns:\nmass modifications: unmodified & oxidation\nchromatography_column_type: \n\n\n\nData description\nThe ProteomeTools project aims to derive molecular and digital tools from the human proteome to facilitate biomedical and life science research. Here, we describe the generation and multimodal LC-MS/MS analysis of >350,000 synthetic tryptic peptides representing nearly all canonical human gene products. This resource will be extended to 1.4 million peptides within two years and all data will be made available to the public in ProteomicsDB.\n\n\nSample protocol description\nTryptic peptides were individually synthesized by solid phase synthesis, combined into pools of ~1,000 peptides and measured on an Orbitrap Fusion mass spectrometer. For each peptide pool, an inclusion list was generated to target peptides for fragmentation in further LC-MS experiments using five fragmentation methods (HCD, CID, ETD, EThCD, ETciD) with ion trap or Orbitrap readout and HCD spectra were recorded at 6 different collision energies.\n\n\nData analysis protocol\nLC-MS runs were individually analyzed using MaxQuant 1.5.3.30.\n\n\nComments\n\nSubset FigShare\nFull FigShare\nTrained Model FigShare\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "code-of-conduct.html",
    "href": "code-of-conduct.html",
    "title": "ProteomicsML",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others‚Äô private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at eric.deutsch@isbscience.org. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla‚Äôs code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "ProteomicsML provides ready-made datasets for machine learning models accompanied by tutorials on how to work with even the most complex data types in the field of proteomics. The resource is set up to evolve together with the field, and we welcome everyone to contribute to the project by adding new datasets and accompanying notebooks.\nProteomicsML was set up as a joint effort of SDU, CompOmics, LUMC, PeptideAtlas, NIST, PRIDE, and MSAID. We believe that ProteomicsML is solid step forward for the field towards more open and reproducible science!\nLearn üìí Explore all tutorials and datasets üôè Ask or answer questions about the tutorials in Tutorials Q&A\nDiscuss üìÑ Discuss the existing datasets or the addition of a new dataset in Dataset Discussions üí¨ Join the ProteomicsML General Discussions\nContribute üí° Have an idea on how to improve the project? Open an issue üßë‚Äçüîß Learn how to Contribute ü§ù Read the Code of Conduct\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "ProteomicsML",
    "section": "",
    "text": "This document describes how to contribute to the ProteomicsML resource by adding new or updating existing tutorials and/or datasets.\n\n\nAt ProteomicsML, we pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. By interacting with or contributing to ProteomicsML at https://github.com/ProteomicsML or at https://proteomicsml.org, you agree to our Code of Conduct. Violation of our Code of Conduct may ultimately lead to a permanent ban from any sort of public interaction within the community. ü§ù Read the Code of Conduct\nIf you have an idea for a new tutorial or dataset, or found a mistake, you are welcome to communicate it with the community by opening a discussion thread in GitHub Discussions or by creating an issue. üí¨ Start a discussion thread üí° Open an issue\n\n\n\nProteomicsML uses the Quarto system to publish a static website from markdown and Jupyter IPython notebook files. All source files are maintained at ProteomicsML/ProteomicsML. Upon each commit on the main branch (after merging a pull request), the website is rebuilt on GitHub Actions and pushed to the ProteomicsML/proteomicsml.github.io repository, where it is hosted with GitHub Pages on the ProteomicsML.org website. See Website deployment for the full deployment workflow.\n\n\n\n\n\n\nFork ProteomicsML/ProteomicsML on GitHub to make your changes.\nClone your fork of the repository to your local machine.\nInstall Quarto to build the website on your machine.\nTo preview the website while editing, run: quarto preview . --render html\n\nMaintainers with write access to the repository can skip the first two steps and make a new local branch instead. Direct commits to the main branch are not allowed.\n\n\n\nProteomicsML tutorials are educational Jupyter notebooks that combine fully functional code cells and descriptive text cells. The end result should be a notebook that is easy to comprehend to anyone with a basic understanding of proteomics, programming, and machine learning. When adding or updating a tutorial, please follow these rules and conventions:\n\nTitle, filename, metadata, and subheadings\n\nTutorials are grouped by data type: Fragmentation, Ion mobility, or Retention time. Place your tutorial notebook in the appropriate directory in the repository. E.g., tutorials/fragmentation. If your tutorial is part of a new data type group, please open a new discussion thread first.\nThe filename should be an abbreviated version of the tutorial title, formatted in kebab case (lowercase with - replacing spaces), for instance title-of-tutorial.ipynb.\nThe following front matter metadata items are required (see the Quarto Documentation for more info):\n\ntitle: A descriptive sentence-like title\nauthors: All authors that significantly contributed to the tutorial\ndate: Use last-modified to automatically render the correct date\n\n\n\n\n\n\n\nNote\n\n\n\nUnfortunately, YAML front matter is not rendered by Google Colab. Instead it is interpreted as plain markdown and the first cell of the notebook might look out of place when loaded into Google Colab. Nevertheless, the front matter results in a clean header on ProteomicsML.org, the primary platform for viewing tutorials.\n\n\nQuarto will render the title automatically from the metadata. Therefore, only subheadings should be included as markdown, starting at the second heading level (##).\nAdd an Open with Colab badge directly after the front matter metadata. The badge should be hyperlinked to open the notebook in Colab directly from GitHub. This can be achieved by replacing https://github.com/ with https://colab.research.google.com/github/ in the full URL to the file on GitHub. Additionally, in this URL the filename should be prefixed with an underscore (_); see point 2 in Website deployment for more info on notebook copies for Colab.\nFor example:\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProteomicsML/ProteomicsML/blob/main/tutorials/fragmentation/_nist-1-parsing-spectral-library.ipynb)\nrenders as\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe URL will not work (or be updated) until the pull request adding or updating the notebook is merged into the main branch.\n\n\n\nSubject and contents\n\nEach tutorial should clearly show and describe one or more steps in a certain machine learning workflow for proteomics.\nSufficiently describe each code cell and each step in the workflow.\nTutorials should ideally be linked to a single ProteomicsML dataset from the same group.\nWhile multiple tutorials can be added for a single data type, make sure that each tutorial is sufficiently different from the others in terms of methodology and/or datasets used.\nReferences\n\nCode cells and programming language\n\nTutorials should work on all major platforms (Linux, Windows, macOS). An exception to this rule can be made if one or more tools central to the tutorial is not cross-platform.\nPer ProteomicsML convention, tutorials should use the Python programming language. Exceptions may be allowed if the other language is essential to the tutorial or methodology.\nProteomicsML recommends Google Colab to interactively use tutorial notebooks. Therefore, all code should be backwards compatible with the Python version used by Google Colab. At time of writing, this is Python 3.7.\nDependencies should ideally be installable with pip. A first code cell can be used to install all requirements using the Jupyter shell prefix !. For instance: ! pip install pandas.\nCode should be easy to read. For Python, follow the PEP8 style guide where possible.\nUpon pull request (PR) creation, all expected output cells should be present. When rendering the ProteomicsML website, notebooks are not rerun. Therefore, as a final step before submitting your PR, restart the kernel, run all cells from start to finish, and save the notebook. See point 2 in Website deployment for more info on notebook copies for Colab.\n\n\n\n\n\nProteomicsML datasets are community-curated proteomics datasets fit for machine learning. Ideally, each dataset is accompanied by a tutorial. When adding or updating a dataset, please follow these rules and conventions:\n\nDataset description and data files:\n\nEach dataset is represented as a single markdown file describing the dataset.\nThe data itself can be added in one of three ways:\n\nIf the dataset itself consists of one or more files, each smaller than 50 MB, they can be added in a subfolder with the same name as the markdown file. These files should be individually gzipped to save space and to prevent line-by-line tracking by Git.\n\n\n\n\n\n\nNote\n\n\n\nGzipped CSV files can very easily be read by Pandas into a DataFrame. Simply use the filename with the .gz suffix in the pandas.read_csv() function and Pandas will automatically unzip the file while reading.\n\n\nLarger files can be added to the ProteomicsML FTP file server by the project maintainers. Please request this in your pull request.\nFiles that are already publicly and persistently stored elsewhere, can be represented by solely the markdown file. In this case, all tutorials using this dataset should start from the file(s) as is and include any required preprocessing steps.\n\n\nTitle, filename, and metadata:\n\nDatasets are grouped by data type: Fragmentation, Ion mobility, or Retention time. Place your dataset and markdown description in the appropriate directory in the repository. E.g., tutorials/fragmentation. If your dataset is part of a new data type group, please open a new discussion thread first.\nThe filename / directory name should be an abbreviated version of the dataset title, formatted in kebab case (lowercase with - replacing spaces), for instance title-of-dataset.md / title-of-dataset/.\nThe following front matter metadata items are required (see the Quarto Documentation for more info):\n\ntitle: A descriptive sentence-like title\ndate: Use last-modified to automatically render the correct date\n\nQuarto will render the title automatically from the metadata. Therefore, only subheadings should be included as markdown, starting at the second heading level (##).\n\nDataset description\n\nAttributes [TODO]\nDescription categories [TODO]\n\n\n\n\n\n\nCommit and push your changes to your fork.\nOpen a pull request with these changes. Choose the pull request template that fits your changes best.\nThe pull request should pass all the continuous integration tests which are automatically run by GitHub Actions.\n\n\n\n\n\nWhen a pull request is merged with the main branch, the following GitHub Actions are triggered:\n\nTest website rendering: The full website is rendered to check that no errors occur. This action should already have been run successfully for the pull request that implemented the changes. Nevertheless, merging could also introduce new issues.\nUpdate notebook copies: A script is run to make copies of all tutorial notebooks with all output removed. The filenames of these copies are prepended with an underscore and should be used to open the notebooks interactively, e.g., in Google Colab.\nPublish website: Quarto is used to render the static website, which is then force-pushed to the ProteomicsML/proteomicsml.github.io repository. This repository is served on proteomicsml.org through GitHub Pages.\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "On ProteomicsML you will find detailed tutorials outlining how to work the latest state-of-the-art machine learning models, and even how to turn your own raw data into a suitable format. Explore all tutorials on ProteomicsML and click the ‚ÄúOpen in Colab‚Äù badge to interact with the notebooks in a userfriendly coding environment.\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/index.html#fragmentation",
    "href": "tutorials/index.html#fragmentation",
    "title": "Tutorials",
    "section": "Fragmentation",
    "text": "Fragmentation\n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\nDate\n\n\n\n\n\n\nNIST (part 1): Parsing the spectral library\n\n\nRalf Gabriels\n\n\nSep 22, 2022\n\n\n\n\nNIST (part 2): Traditional ML: Gradient boosting\n\n\nRalf Gabriels\n\n\nSep 22, 2022\n\n\n\n\nNIST (part 3): Deep learning: BiLSTM\n\n\nRalf Gabriels\n\n\nSep 22, 2022\n\n\n\n\nProsit-style GRU with pre-annotated ProteomeTools data\n\n\nSiegfried Gessulat\n\n\nSep 22, 2022\n\n\n\n\nRaw file processing with PROSIT style annotation\n\n\nTobias Greisager Rehfeldt\n\n\nSep 22, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/index.html#ion-mobility",
    "href": "tutorials/index.html#ion-mobility",
    "title": "Tutorials",
    "section": "Ion mobility",
    "text": "Ion mobility\n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\nDate\n\n\n\n\n\n\nPredicting CCS values for TIMS data\n\n\nRobbin Bouwmeester\n\n\nSep 22, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/index.html#retention-time",
    "href": "tutorials/index.html#retention-time",
    "title": "Tutorials",
    "section": "Retention time",
    "text": "Retention time\n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\nDate\n\n\n\n\n\n\nDLOmix embedding of Prosit model on ProteomeTools data\n\n\nTobias Greisager Rehfeldt\n\n\nSep 22, 2022\n\n\n\n\nManual embedding of Bi-LSTM model on ProteomeTools data\n\n\nTobias Greisager Rehfeldt\n\n\nSep 22, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/proteinvisibility/Modeling protein detectability.html",
    "href": "tutorials/proteinvisibility/Modeling protein detectability.html",
    "title": "ProteomicsML",
    "section": "",
    "text": "Introduction\nWhen subjecting whole cell lysates to mass spectrometry-based proteomics analysis, some proteins are easily detected while others are not seen. The proteins that are never detected are often colloquially called the dark proteome. There are many reasons for not detecting proteins. Some proteins may only be found in certain cell types or in certain developmental stages. Comprehensive accumulation of datasets from different cell types and developmental stages can overcome this limitation. Other reasons such as the physicochemical properties of the proteins may hinder detection. Here we explore the ‚Äúlight and dark proteome‚Äù based on proteins that are observed and not observed in the Arabidopsis PeptideAtlas, which has been assembled by search over 200 million MS/MS spectra from 100 different datasets.\nFirst we import some needed libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\n\nRead input data and extract the columns to train on. We will attempt to train on the protein molecular weight, protein physicochemical properties gravy score (a measure of hydrophobicity), isoelectric point (pI), and then two metrics from RNA-seq analysis: the percentage of RNA-seq experiments that detect a transcript for the given protein, and the highest TPM (transcripts per million, i.e.¬†abundance) in any one dataset.\n\nproteins = pd.read_csv('http://www.peptideatlas.org/builds/arabidopsis/light_and_dark_protein_list.tsv',sep=\"\\t\")\nlearning_values = proteins[ ['molecular_weight', 'gravy', 'pI', 'rna_detected_percent', 'highest_tpm' ] ].copy()\n\nNormalize the data to have ranges like 0 to 1\n\nlearning_values.loc[ :, 'molecular_weight'] = learning_values['molecular_weight'] / 100\nlearning_values.loc[ learning_values[ 'molecular_weight'] > 1, 'molecular_weight'] = 1.0\nlearning_values.loc[ :, 'gravy'] = ( learning_values['gravy'] + 2 ) / 4\nlearning_values.loc[ :, 'pI'] = ( learning_values['pI'] - 4 ) / 8\n\nlearning_values.loc[ :, 'rna_detected_percent'] = learning_values['rna_detected_percent'] / 100\nlearning_values.loc[ :, 'highest_tpm'] = learning_values['highest_tpm'] / 300\nlearning_values.loc[ learning_values[ 'highest_tpm'] > 1, 'highest_tpm'] = 1.0\n\nSet the classifications to 0 and 1\n\nclasses = proteins['status'].copy()\nclasses[ classes == 'canonical' ] = 1\nclasses[ classes == 'not observed' ] = 0\n\nSplit into 75% train and 25% test\n\nX_train, X_test, y_train, y_test = train_test_split(learning_values, classes, test_size=0.25)\n\nTrain the classifier on the training set\n\nclf = MLPClassifier(solver='lbfgs', max_iter=1000, hidden_layer_sizes=(100,), alpha=1e-4, random_state=1)\nclf.fit(X_train, list(y_train))\n\nC:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n\n\nMLPClassifier(max_iter=1000, random_state=1, solver='lbfgs')\n\n\nPredict for all the test set\n\npredictions = clf.predict(X_test)\nprobabilities = clf.predict_proba(X_test)\n\nMake a ROC curve\n\nprobabilities_list = list(probabilities[ :, 1])\nfpr, tpr, thresholds = roc_curve(np.ravel(list(y_test)), np.ravel(probabilities_list))\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr,tpr,color=\"darkorange\",lw=2,label=\"ROC curve (area = %0.2f)\" % roc_auc)\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC plot for canonical predictions\")\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()\n\n\n\n\nPredict for all protein and write out the table with learned results\n\nprobabilities = clf.predict_proba(learning_values)\nproteins['learned_canonical_prob'] = probabilities[ :, 1]\nproteins.to_csv('light_and_dark_protein_list_trained.tsv', sep=\"\\t\", index=False)\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/ionmobility/index.html",
    "href": "tutorials/ionmobility/index.html",
    "title": "Ion mobility",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/ionmobility/meier-tims-ccs.html",
    "href": "tutorials/ionmobility/meier-tims-ccs.html",
    "title": "Predicting CCS values for TIMS data",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/ionmobility/meier-tims-ccs.html#introduction",
    "href": "tutorials/ionmobility/meier-tims-ccs.html#introduction",
    "title": "Predicting CCS values for TIMS data",
    "section": "Introduction",
    "text": "Introduction\nIon mobility is a technique to separate ionized analytes based on their size, shape, and physicochemical properties. Initially the techniques for ion mobility propelled the ions with an electric field through a cell with inert gas. The ions collide with the inert gas without fragmentation. Separation is achieved by propelling the ions faster or slower in the electric field (i.e., based on their charge) and are slowed down by the collisions with the gas (i.e., based on shape and size). Trapped ion mobility (TIMS) reverses this operation by trapping the ions in an electric field and forcing them forward by collision with the gas. From any of the different ion mobility techniques you are able to derive the collisional cross section (CCS) in Angstrom squared. In this notebook you can follow a short tutorial on how to train a Machine Learning model for the prediction of these CCS values.\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom collections import Counter\nfrom scipy.stats import pearsonr\n\nvol_dict = {\"A\" : 88.6,\n            \"B\" : 0.0,\n            \"O\" : 0.0,\n            \"X\" : 0.0,\n            \"J\" : 0.0,\n            \"R\" : 173.4,\n            \"N\" : 114.1,\n            \"D\" : 111.1,\n            \"C\" : 108.5,\n            \"Q\" : 143.8,\n            \"E\" : 138.4,\n            \"G\" : 60.1,\n            \"H\" : 153.2,\n            \"I\" : 166.7,\n            \"L\" : 166.7,\n            \"K\" : 168.6,\n            \"M\" : 162.9,\n            \"F\" : 189.9,\n            \"P\" : 112.7,\n            \"S\" : 89.0,\n            \"T\" : 116.1,\n            \"W\" : 227.8,\n            \"Y\" : 193.6,\n            \"V\" : 140}\n\naa_to_pos = dict(zip(vol_dict.keys(),range(len(vol_dict.keys()))))"
  },
  {
    "objectID": "tutorials/ionmobility/meier-tims-ccs.html#data-reading-and-preparation",
    "href": "tutorials/ionmobility/meier-tims-ccs.html#data-reading-and-preparation",
    "title": "Predicting CCS values for TIMS data",
    "section": "Data reading and preparation",
    "text": "Data reading and preparation\nRead the training data from Meier et al.\n\nccs_df = pd.read_csv(\"https://github.com/ProteomicsML/IonMobility/blob/main/datasets/Meier_IM_CCS/combined_sm.zip?raw=true\", compression=\"zip\", index_col=0)\n\nExecute the cell below to read a smaller data set from Van Puyvelde et al.. Remove all the ‚Äú#‚Äù to read this smaller data set. On for example colab it is recommended to load this smaller data set. Please do note that the description is based on the larger data set. It is expected that more complex models do not benefit at the same rate from the smaller data set (e.g., the deep learning network). Hans Vissers from Waters analyzed this traveling wave IM data:\n\n#ccs_df = pd.read_csv(\n#    \"https://raw.githubusercontent.com/ProteomicsML/IonMobility/main/datasets/VanPuyvelde_TWIMS_CCS/TWIMSpeptideCCS.tsv\",\n#    low_memory=False,\n#    sep=\"\\t\"\n#)\n\nA small summarization of the data that was just read:\n\nccs_df.describe()\n\n\n\n\n\n  \n    \n      \n      Charge\n      Mass\n      Intensity\n      Retention time\n      CCS\n    \n  \n  \n    \n      count\n      718917.000000\n      718917.000000\n      7.189170e+05\n      718917.000000\n      718917.000000\n    \n    \n      mean\n      2.376747\n      1829.771049\n      6.716163e+05\n      300.215311\n      475.545205\n    \n    \n      std\n      0.582843\n      606.256496\n      2.139819e+06\n      940.711797\n      109.083740\n    \n    \n      min\n      2.000000\n      696.428259\n      2.790800e+02\n      0.004795\n      275.418854\n    \n    \n      25%\n      2.000000\n      1361.766700\n      5.405700e+04\n      28.260000\n      392.076630\n    \n    \n      50%\n      2.000000\n      1729.834520\n      1.655000e+05\n      50.624000\n      454.656281\n    \n    \n      75%\n      3.000000\n      2189.009920\n      5.357000e+05\n      84.241000\n      534.702698\n    \n    \n      max\n      4.000000\n      4599.284130\n      2.481000e+08\n      6897.700000\n      1118.786133\n    \n  \n\n\n\n\n\nccs_df\n\n\n\n\n\n  \n    \n      \n      Modified sequence\n      Charge\n      Mass\n      Intensity\n      Retention time\n      CCS\n      PT\n    \n  \n  \n    \n      0\n      _(ac)AAAAAAAAAAGAAGGR_\n      2\n      1239.63200\n      149810.0\n      70.140\n      409.092529\n      False\n    \n    \n      1\n      _(ac)AAAAAAAAEQQSSNGPVKK_\n      2\n      1810.91734\n      21349.0\n      19.645\n      481.229248\n      True\n    \n    \n      2\n      _(ac)AAAAAAAGAAGSAAPAAAAGAPGSGGAPSGSQGVLIGDR_\n      3\n      3144.55482\n      194000.0\n      3947.700\n      772.098083\n      False\n    \n    \n      3\n      _(ac)AAAAAAAGDSDSWDADAFSVEDPVRK_\n      2\n      2634.18340\n      6416400.0\n      94.079\n      573.213196\n      False\n    \n    \n      4\n      _(ac)AAAAAAAGDSDSWDADAFSVEDPVRK_\n      3\n      2634.18340\n      5400600.0\n      94.841\n      635.000549\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      718912\n      _YYYVCQYCPAGNNM(ox)NR_\n      2\n      2087.82880\n      131230.0\n      21.753\n      461.667145\n      True\n    \n    \n      718913\n      _YYYVCQYCPAGNWANR_\n      2\n      2083.86690\n      84261.0\n      28.752\n      459.721191\n      True\n    \n    \n      718914\n      _YYYVPADFVEYEK_\n      2\n      1684.76609\n      382810.0\n      92.273\n      436.103699\n      False\n    \n    \n      718915\n      _YYYVQNVYTPVDEHVYPDHR_\n      3\n      2556.17099\n      30113.0\n      26.381\n      580.297058\n      True\n    \n    \n      718916\n      _YYYVQNVYTPVDEHVYPDHR_\n      4\n      2556.17099\n      33682.0\n      26.381\n      691.901123\n      True\n    \n  \n\n718917 rows √ó 7 columns\n\n\n\nPrepare the data to not contain any \"_\" characters or modifications in between [ ]:\n\n# Strip \"_\" from sequence\nccs_df[\"sequence\"] = ccs_df[\"Modified sequence\"].str.strip(\"_\")\n\n# Strip everything between \"()\" and \"[]\" from sequence\nccs_df[\"sequence\"] = ccs_df[\"sequence\"].str.replace(r\"[\\(\\[].*?[\\)\\]]\", \"\", regex=True)\n\nCount the occurence of amino acids, those that did not get detected; repace with 0\n\n# Apply counter to each sequence, fill NA with 0.0, make matrix from counts\nX_matrix_count = pd.DataFrame(ccs_df[\"sequence\"].apply(Counter).to_dict()).fillna(0.0).T\n\n\nX_matrix_count\n\n\n\n\n\n  \n    \n      \n      A\n      G\n      R\n      E\n      Q\n      S\n      N\n      P\n      V\n      K\n      L\n      I\n      D\n      W\n      F\n      M\n      T\n      C\n      Y\n      H\n    \n  \n  \n    \n      0\n      12.0\n      3.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      8.0\n      1.0\n      0.0\n      1.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1.0\n      2.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      17.0\n      9.0\n      1.0\n      0.0\n      1.0\n      4.0\n      0.0\n      3.0\n      1.0\n      0.0\n      1.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      9.0\n      1.0\n      1.0\n      1.0\n      0.0\n      3.0\n      0.0\n      1.0\n      2.0\n      1.0\n      0.0\n      0.0\n      5.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      9.0\n      1.0\n      1.0\n      1.0\n      0.0\n      3.0\n      0.0\n      1.0\n      2.0\n      1.0\n      0.0\n      0.0\n      5.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      718912\n      1.0\n      1.0\n      1.0\n      0.0\n      1.0\n      0.0\n      3.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      2.0\n      4.0\n      0.0\n    \n    \n      718913\n      2.0\n      1.0\n      1.0\n      0.0\n      1.0\n      0.0\n      2.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      2.0\n      4.0\n      0.0\n    \n    \n      718914\n      1.0\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      0.0\n      1.0\n      2.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      4.0\n      0.0\n    \n    \n      718915\n      0.0\n      0.0\n      1.0\n      1.0\n      1.0\n      0.0\n      1.0\n      2.0\n      4.0\n      0.0\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      5.0\n      2.0\n    \n    \n      718916\n      0.0\n      0.0\n      1.0\n      1.0\n      1.0\n      0.0\n      1.0\n      2.0\n      4.0\n      0.0\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      5.0\n      2.0\n    \n  \n\n718917 rows √ó 20 columns\n\n\n\nA fairly rudimentary technique is to use the volume of each amino acid and sum these volumes:\n\ndef to_predicted_ccs(row):\n    vol_sum = sum([vol_dict[k]*v for k,v in row.to_dict().items()])    \n    return vol_sum\n\nccs_df[\"predicted_CCS_vol_based\"] = X_matrix_count.apply(to_predicted_ccs,axis=1)\n\nLets see the results:\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\nfor c in range(2,5):\n    plt.scatter(\n        ccs_df.loc[ccs_df[\"Charge\"]==c,\"CCS\"],\n        ccs_df.loc[ccs_df[\"Charge\"]==c,\"predicted_CCS_vol_based\"],\n        alpha=set_alpha,\n        s=set_size, \n        label=\"Z=\"+str(c)\n    )\n\nlegend = plt.legend()\n\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n\nplt.xlabel(\"Observed CCS (Angstrom^2)\")\nplt.xlabel(\"Predicted CCS (Angstrom^2)\")\n    \nplt.show()\n\n\n\n\nClear correlation, but seems we need to change the intercepts of each curve and make seperate predictions for each peptide charge state. In addition to these observations it seems that higher charge states have higher errors. This likely influenced by a large part by the relation between higher charge states and longer peptides. These longer peptides can deviate more from each other in terms of structures (and CCS). Instead of spending more time on this, lets have a look at a more ML-based approach."
  },
  {
    "objectID": "tutorials/ionmobility/meier-tims-ccs.html#training-a-linear-regression-model-for-ccs-prediction",
    "href": "tutorials/ionmobility/meier-tims-ccs.html#training-a-linear-regression-model-for-ccs-prediction",
    "title": "Predicting CCS values for TIMS data",
    "section": "Training a linear regression model for CCS prediction",
    "text": "Training a linear regression model for CCS prediction\n\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport random\n\nIn this section we will fit a linear regression model. This model is only able to fit a linear function between the features (sequence) and target (CCS). This linear model can be expressed as the following equation:\n$ Y = _0 + _1 X$\nWhere \\(Y\\) is a vector (/list) of all CCS values and X a matrix (/2-dimensional list) of all the amino acids counts. The intercept and weights of each features are learned so the predicted value (\\(\\hat{Y}\\)) is close to the observed outcome (\\(Y\\)). What is considered close and how this closeness between predictions and observations are minimized is not further discussed here. However, there is a rich amount of information available on the internet (e.g., https://www.coursera.org/learn/machine-learning).\nFirst, we will split the matrix into 90% training peptides and 10% testing peptides. These testing peptides are very valuable in estimating model performance. Since the model has not seen these sequences before it cannot overfit on these particular examples.\n\n# Get all the index identifiers\nall_idx = list(X_matrix_count.index)\nrandom.seed(42)\n\n# Shuffle the index identifiers so we can randomly split them in a testing and training set\nrandom.shuffle(all_idx)\n\n# Select 90 % for training and the remaining 10 % for testing\ntrain_idx = all_idx[0:int(len(all_idx)*0.9)]\ntest_idx = all_idx[int(len(all_idx)*0.9):]\n\n# Get the train and test indices and point to new variables\nccs_df_train = ccs_df.loc[train_idx,:]\nccs_df_test = ccs_df.loc[test_idx,:]\n\n# Also for the feature matrix get the train and test indices\nX_matrix_count_train = X_matrix_count.loc[train_idx,:]\nX_matrix_count_test = X_matrix_count.loc[test_idx,:]\n\nNow lets start training the models. Although we could encode the charge as a feature here we separate all models to counter any charge to composition specific patterns.\n\n# Initialize a model object\nlinear_model_z2 = LinearRegression()\n\n# Fit the initialized model object to our training data (only charge 2)\nlinear_model_z2.fit(\n    X=X_matrix_count_train.loc[ccs_df_train[\"Charge\"]==2,:],\n    y=ccs_df_train.loc[ccs_df_train[\"Charge\"]==2,\"CCS\"]\n)\n\n# Repeat for the other two charge states\nlinear_model_z3 = LinearRegression()\nlinear_model_z3.fit(\n    X=X_matrix_count_train.loc[ccs_df_train[\"Charge\"]==3,:],\n    y=ccs_df_train.loc[ccs_df_train[\"Charge\"]==3,\"CCS\"]\n)\n\nlinear_model_z4 = LinearRegression()\nlinear_model_z4.fit(\n    X=X_matrix_count_train.loc[ccs_df_train[\"Charge\"]==4,:],\n    y=ccs_df_train.loc[ccs_df_train[\"Charge\"]==4,\"CCS\"]\n)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nNow we can have a look at the coefficients \\(\\beta_1\\) learned. These should be highly correlated with the previous experimentally determined volumetric observations for each amino acid:\n\n# Scatter plot the coefficients of each amino acid against their experimentally determined volumes\nplt.scatter(\n    linear_model_z2.coef_,\n    [vol_dict[v] for v in X_matrix_count.columns]\n)\n\n# Plot a diagonal line we expect the points to be on\nplt.plot(\n    [6.0,26.0],\n    [60.0,260],\n    c=\"grey\",\n    zorder=0\n)\n\n# Annotate each point with their respective amino acids\nfor v,x,y in zip(X_matrix_count.columns,\n                 linear_model_z2.coef_,\n                 [vol_dict[v] for v in X_matrix_count.columns]):\n    \n    plt.annotate(v,(x+0.1,y+5.0))\n    \nplt.show()\n\n\n\n\nObservations are very similar. There are differences that could be cause by a multitude of reasons. For example, the difference between volumetric observations in the CCS cell is different or being part of a polypeptide chain changes the volume of the amino acid.\nNext we will plot the predictions of the test set and compare them with observational data. Note that we apply each charge model seperately.\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\n# Scatter plot the observations on the test set against the predictions on the same set (z=2)\nplt.scatter(\n    linear_model_z2.predict(X=X_matrix_count_test.loc[ccs_df[\"Charge\"]==2,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==2,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=2\"\n)\n\n# Scatter plot the observations on the test set against the predictions on the same set (z=3)\nplt.scatter(\n    linear_model_z3.predict(X=X_matrix_count_test.loc[ccs_df[\"Charge\"]==3,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=3\"\n)\n\n# Scatter plot the observations on the test set against the predictions on the same set (z=4)\nplt.scatter(\n    linear_model_z4.predict(X=X_matrix_count_test.loc[ccs_df[\"Charge\"]==4,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==4,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=4\"\n)\n\n# Plot a diagonal the points should be one\nplt.plot([300,1100],[300,1100],c=\"grey\")\n\n# Add a legend for the charge states\nlegend = plt.legend()\n\n# Make sure the legend labels are visible and big enough\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n    \n# Get the predictions and calculate performance metrics\npredictions = linear_model_z2.predict(X_matrix_count_test.loc[ccs_df[\"Charge\"]==3,:])\nmare = round(sum((abs(predictions-ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"])/ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"])*100)/len(predictions),3)\npcc = round(pearsonr(predictions,ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"])[0],3)\nperc_95 = round(np.percentile((abs(predictions-ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"])/ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"])*100,95)*2,2)\n\nplt.title(f\"Linear model - PCC: {pcc} - MARE: {mare}% - 95th percentile: {perc_95}% (z3 model for z3 observations)\")\n\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel(\"Observed CCS (^2)\")\nplt.ylabel(\"Predicted CCS (^2)\")\n\nplt.show()\n\n\n\n\nIt is clear that the predictions and observations are on the diagonal. This means that they are very similar. However, there are still some differences between observations and predictions.\nIn the previous example we trained models for charge state seperately. This is slightly inconvenient and other charge states might still be able to provide useful training examples. As long as the model corrects for the right charge state of course. In the next example we add charge state to the feature matrix. The linear model should be (partially‚Ä¶) able to account for the charge states of peptides.\n\n# Make a new copy of feature matrix and add charge as a feature\nX_matrix_count_charge_train = X_matrix_count_train.copy()\nX_matrix_count_charge_train[\"charge\"] = ccs_df_train[\"Charge\"]\n\nX_matrix_count_charge_test = X_matrix_count_test.copy()\nX_matrix_count_charge_test[\"charge\"] = ccs_df_test[\"Charge\"]\n\n\n# Fit the linear model, but this time with the charge as a feature\nlinear_model = LinearRegression()\n\nlinear_model.fit(\n    X=X_matrix_count_charge_train,\n    y=ccs_df_train.loc[:,\"CCS\"]\n)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\n# Scatter plot the observations on the test set against the predictions on the same set\nplt.scatter(\n    linear_model.predict(X=X_matrix_count_charge_test.loc[ccs_df[\"Charge\"]==2,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==2,\"CCS\"],\n    alpha=set_alpha,\n    s=1,\n    label=\"Z=2\"\n)\n\nplt.scatter(\n    linear_model.predict(X=X_matrix_count_charge_test.loc[ccs_df[\"Charge\"]==3,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"],\n    alpha=set_alpha,\n    s=1,\n    label=\"Z=3\"\n)\n\nplt.scatter(\n    linear_model.predict(X=X_matrix_count_charge_test.loc[ccs_df[\"Charge\"]==4,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==4,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=4\"\n)\n\n# Plot a diagonal the points should be one\nplt.plot([300,1100],[300,1100],c=\"grey\")\n\nlegend = plt.legend()\n\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n\n# Get the predictions and calculate performance metrics\npredictions = linear_model.predict(X=X_matrix_count_charge_test)\nmare = round(sum((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100)/len(predictions),3)\npcc = round(pearsonr(predictions,ccs_df_test.loc[:,\"CCS\"])[0],3)\nperc_95 = round(np.percentile((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100,95)*2,2)\n\nplt.title(f\"Linear model - PCC: {pcc} - MARE: {mare}% - 95th percentile: {perc_95}%\")\n\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel(\"Observed CCS (^2)\")\nplt.ylabel(\"Predicted CCS (^2)\")\n    \nplt.show()\n\n\n\n\nWith this model we are capable to predict CCS values for all three charge states (maybe more; be careful with extrapolation). However, it also shows that both z3 and z4 are not optimally predicted. Especially z4 we can probably draw a line manually that provides better performance than the current model. The incapability of the model to correctly predict some of these values is largely due to the linear algorithm. With this algorithm we can only fit ‚Äúsimple‚Äù linear relations, but more complex relations are not modeled correctly. In the next section we will fit a non-linear model that is able to capture these complex relations better. However, keep in mind that more complex models are usually also able to overfit data better, resulting in poorer generalization performance."
  },
  {
    "objectID": "tutorials/ionmobility/meier-tims-ccs.html#training-an-rf-non-linear-regression-model-for-ccs-prediction",
    "href": "tutorials/ionmobility/meier-tims-ccs.html#training-an-rf-non-linear-regression-model-for-ccs-prediction",
    "title": "Predicting CCS values for TIMS data",
    "section": "Training an RF (non-linear) regression model for CCS prediction",
    "text": "Training an RF (non-linear) regression model for CCS prediction\nIn this section we will fit a random forest (RF) regression model. We hope to fit some of the non-linear relations present in the data. The RF algorithm fits multiple decision trees, but what makes these trees different is the random selection of instances (peptides) and/or features (amino acid count). The predictions between the forest of trees can be averaged to obtain a single prediction per peptide (instead of multiple for the same peptide). Later we will see that the algorithm might actually not be suitable for fitting this type of data.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n# Make a new copy of feature matrix and add charge as a feature\nX_matrix_count_charge_train = X_matrix_count_train.copy()\nX_matrix_count_charge_train[\"charge\"] = ccs_df_train[\"Charge\"]\n\nX_matrix_count_charge_test = X_matrix_count_test.copy()\nX_matrix_count_charge_test[\"charge\"] = ccs_df_test[\"Charge\"]\n\n\n# Initialize a RF object, note the hyperparameters that the model will follow\nrf_model = RandomForestRegressor(\n                max_depth=20,\n                n_estimators=50,\n                n_jobs=-1\n)\n\n# Fit the RF model\nrf_model.fit(\n    X=X_matrix_count_charge_train,\n    y=ccs_df_train.loc[:,\"CCS\"]\n)\n\nRandomForestRegressor(max_depth=20, n_estimators=50, n_jobs=-1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=20, n_estimators=50, n_jobs=-1)\n\n\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\n# Scatter plot the observations on the test set against the predictions on the same set\nplt.scatter(\n    rf_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==2,:]),\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==2,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=2\"\n)\n\nplt.scatter(\n    rf_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==3,:]),\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==3,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=3\"\n)\n\nplt.scatter(\n    rf_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==4,:]),\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==4,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=4\"\n)\n\n# Plot a diagonal the points should be one\nplt.plot([300,1100],[300,1100],c=\"grey\")\n\nlegend = plt.legend()\n\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n\n# Get the predictions and calculate performance metrics\npredictions = rf_model.predict(X=X_matrix_count_charge_test)\nmare = round(sum((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100)/len(predictions),3)\npcc = round(pearsonr(predictions,ccs_df_test.loc[:,\"CCS\"])[0],3)\nperc_95 = round(np.percentile((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100,95)*2,2)\n\nplt.title(f\"RF - PCC: {pcc} - MARE: {mare}% - 95th percentile: {perc_95}%\")\n\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel(\"Observed CCS (^2)\")\nplt.ylabel(\"Predicted CCS (^2)\")\n\nplt.show()\n\n\n\n\nAs can be observed the problem with z=4 splitting up is gone, probably due to the capability of RF to fit non-linear relations. However, we see quite a large deviation on the diagonal. One of the major causes of this problem is the exclusion of amino acid counts for the decision trees. Although this is fundamental to the inner workings of RF, it means that we cannot take the excluded amino acids into account and these values are likely to be replaced by average expected volume to other (non-excluded) amino acids. RF performs very well when features correlate, and predictions are not fully dependent on the inclusion of all features. Next we will look at a decision tree algorithm (XGBoost) that does not rely on the exclusion of features.\nPS note that you might be able to fit a much better model by using a much larger number of trees, but overall the problem largely remains, and it is better to choose an algorithm that respects/fits your data best."
  },
  {
    "objectID": "tutorials/ionmobility/meier-tims-ccs.html#training-a-xgboost-non-linear-regression-model-for-ccs-prediction",
    "href": "tutorials/ionmobility/meier-tims-ccs.html#training-a-xgboost-non-linear-regression-model-for-ccs-prediction",
    "title": "Predicting CCS values for TIMS data",
    "section": "Training a XGBoost (non-linear) regression model for CCS prediction",
    "text": "Training a XGBoost (non-linear) regression model for CCS prediction\nIn this section we will fit a XGBoost regression model. This algorithm works by training a sequence of underfitted models. Each model in the sequence receives the output of the previous decision tree models. This combination of trees allows to fit the data well without greatly overfitting it.\n\nfrom xgboost import XGBRegressor\n\n\n# Make a new copy of feature matrix and add charge as a feature\nX_matrix_count_charge_train = X_matrix_count_train.copy()\nX_matrix_count_charge_train[\"charge\"] = ccs_df_train[\"Charge\"]\n\nX_matrix_count_charge_test = X_matrix_count_test.copy()\nX_matrix_count_charge_test[\"charge\"] = ccs_df_test[\"Charge\"]\n\n\n# Initialize the XGB object\nxgb_model = XGBRegressor(\n                max_depth=12,\n                n_estimators=250\n)\n\n# Fit the XGB model\nxgb_model.fit(\n    X=X_matrix_count_charge_train,\n    y=ccs_df_train.loc[:,\"CCS\"]\n)\n\nXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             early_stopping_rounds=None, enable_categorical=False,\n             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=12, max_leaves=0, min_child_weight=1,\n             missing=nan, monotone_constraints='()', n_estimators=250, n_jobs=0,\n             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n             reg_lambda=1, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             early_stopping_rounds=None, enable_categorical=False,\n             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=12, max_leaves=0, min_child_weight=1,\n             missing=nan, monotone_constraints='()', n_estimators=250, n_jobs=0,\n             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n             reg_lambda=1, ...)\n\n\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\n# Scatter plot the observations on the test set against the predictions on the same set\nplt.scatter(\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==2,\"CCS\"],\n    xgb_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==2,:]),\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=2\")\n\nplt.scatter(\n    xgb_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==3,:]),\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==3,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=3\"\n)\n\nplt.scatter(\n    xgb_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==4,:]),\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==4,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=4\"\n)\n\n# Plot a diagonal the points should be one\nplt.plot([300,1100],[300,1100],c=\"grey\")\n\nlegend = plt.legend()\n\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n    \n# Get the predictions and calculate performance metrics\npredictions = xgb_model.predict(X_matrix_count_charge_test)\nmare = round(sum((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100)/len(predictions),3)\npcc = round(pearsonr(predictions,ccs_df_test.loc[:,\"CCS\"])[0],3)\nperc_95 = round(np.percentile((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100,95)*2,2)\n\nplt.title(f\"XGBoost - PCC: {pcc} - MARE: {mare}% - 95th percentile: {perc_95}%\")\n\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel(\"Observed CCS (^2)\")\nplt.ylabel(\"Predicted CCS (^2)\")\n\nplt.show()"
  },
  {
    "objectID": "tutorials/ionmobility/meier-tims-ccs.html#training-a-deep-learning-lstm-model-for-ccs-prediction",
    "href": "tutorials/ionmobility/meier-tims-ccs.html#training-a-deep-learning-lstm-model-for-ccs-prediction",
    "title": "Predicting CCS values for TIMS data",
    "section": "Training a deep learning LSTM model for CCS prediction",
    "text": "Training a deep learning LSTM model for CCS prediction\nThe deviation on the diagonal has been decreased significantly. But‚Ä¶ A decision tree based algorithm is usually not the best for a regression model. Since the target data is continuous a model that can respect this structure is likely to perform better. Furthermore, up till now we simply counted amino acids, but structure is important. So to get the most out of the data we need to use the exact positions of amino acids.\nAlso‚Ä¶ We have a lot of data it makes sense to use deep learning (DL). DL models are usually capable of learning more complex relations than traditional algorithms. Furthormore, for traditional ML algorithms we usually need to engineer features, while DL can usually work directly from raw data. DL is able to construct its own features.\n\nfrom tensorflow.keras.layers import Dense, concatenate, Input, Bidirectional, LSTM\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\n\nAs mentioned before, we want to use features that can also tell us something about the potential structure of the peptide. This means we need to take the sequence of the peptide into account and not just the amino acid counts. For this we will use a ‚Äòone-hot encoding‚Äô, in this matrix each position in the peptide are the columns (number of columns equals the length of the peptide) and each amino acid per position has its own row (for the standard amino acids this is 20). So as a result we create a matrix that is the length of the peptide by the amount of unique amino acids in the whole data set. For each position we indicate the presence with a ‚Äò1‚Äô and absence with ‚Äò0‚Äô. As a result the sum of each columnn is ‚Äò1‚Äô and the sum of the whole matrix equals the length of the peptide.\n\ndef aa_seq_to_one_hot(seq,padding_length=60):\n    # Although padding is not needed for an LSTM, we might need it if we for example apply a CNN\n    # Calculate how much padding is needed\n    seq_len = len(seq)\n    if seq_len > padding_length:\n        seq = seq[0:padding_length]\n        seq_len = len(seq)\n\n    # Add padding for peptides that are too short\n    padding = \"\".join([\"X\"] * (padding_length - len(seq)))\n    seq = seq + padding\n\n    # Initialize all feature matrix\n    matrix_hc = np.zeros(\n        (len(aa_to_pos.keys()), len(seq)), dtype=np.int8)\n    \n    # Fill the one-hot matrix, when we encounter an 'X' it should be the end of the sequence\n    for idx,aa in enumerate(seq):\n        if aa == \"X\":\n            break\n        matrix_hc[aa_to_pos[aa],idx] = 1\n    \n    return matrix_hc\n\n\n# Calculate the one-hot matrices and stack them\n# Result is a 3D matrix where the first dimension is each peptide, and then the last two dims are the one-hot matrix\none_hot_encoded_train = np.stack(ccs_df_train[\"sequence\"].apply(aa_seq_to_one_hot).values)\none_hot_encoded_test = np.stack(ccs_df_test[\"sequence\"].apply(aa_seq_to_one_hot).values)\n\n\nif len(ccs_df.index) < 1e4:\n    epochs = 100\n    num_lstm = 12\n    batch_size = 128\nelse:\n    batch_size = 1024\n    epochs = 10\n    num_lstm = 64\n\nbatch_size = 128\nv_split    = 0.1\noptimizer  = \"adam\"\nloss       = \"mean_squared_error\"\n\n# The architecture chosen consists of two inputs: (1) the one-hot matrix and (2) the charge\n# The first part is a biderectional LSTM (a), in paralel we have dense layers containing the charge (b)\n# Both a and b are concatenated to go through several dense layers (c)\ninput_a = Input(shape=(None, one_hot_encoded_train.shape[2]))\na = Bidirectional(LSTM(num_lstm,return_sequences=True))(input_a)\na = Bidirectional(LSTM(num_lstm))(a)\na = Model(inputs=input_a, outputs=a)\n\ninput_b = Input(shape=(1,))\nb = Dense(5, activation=\"relu\")(input_b)\nb = Model(inputs=input_b, outputs=b)\n\nc = concatenate([a.output, b.output],axis=-1)\n\nc = Dense(64, activation=\"relu\")(c)\nc = Dense(32, activation=\"relu\")(c)\nc = Dense(1, activation=\"relu\")(c)\n\n# Create the model with specified inputs and outputs\nmodel = Model(inputs=[a.input, b.input], outputs=c)\n\nmodel.compile(optimizer=optimizer, loss=loss)\n\n# Fit the model on the training data\nhistory = model.fit(\n            (one_hot_encoded_train,ccs_df_train.loc[:,\"Charge\"]),\n            ccs_df_train.loc[:,\"CCS\"],\n            epochs=epochs, \n            batch_size=batch_size,\n            validation_split=v_split\n)\n\nEpoch 1/10\n4550/4550 [==============================] - 108s 23ms/step - loss: 5536.7256 - val_loss: 467.1984\nEpoch 2/10\n4550/4550 [==============================] - 102s 22ms/step - loss: 436.4136 - val_loss: 403.9187\nEpoch 3/10\n4550/4550 [==============================] - 100s 22ms/step - loss: 395.7551 - val_loss: 384.5470\nEpoch 4/10\n4550/4550 [==============================] - 103s 23ms/step - loss: 376.4315 - val_loss: 382.0145\nEpoch 5/10\n4550/4550 [==============================] - 102s 23ms/step - loss: 364.8819 - val_loss: 395.8338\nEpoch 6/10\n4550/4550 [==============================] - 106s 23ms/step - loss: 357.4092 - val_loss: 355.8185\nEpoch 7/10\n4550/4550 [==============================] - 104s 23ms/step - loss: 342.5216 - val_loss: 312.9571\nEpoch 8/10\n4550/4550 [==============================] - 104s 23ms/step - loss: 275.4517 - val_loss: 274.8963\nEpoch 9/10\n4550/4550 [==============================] - 110s 24ms/step - loss: 253.2955 - val_loss: 252.0118\nEpoch 10/10\n4550/4550 [==============================] - 105s 23ms/step - loss: 240.6064 - val_loss: 260.1613\n\n\n\n# Predict CCS values test set\nccs_df_test[\"LSTM_predictions\"] = model.predict((one_hot_encoded_test,ccs_df_test.loc[:,\"Charge\"]))\n\n2247/2247 [==============================] - 20s 8ms/step\n\n\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\n# Scatter plot the observations on the test set against the predictions on the same set\nplt.scatter(\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==2,\"CCS\"],\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==2,\"LSTM_predictions\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=2\"\n)\n\nplt.scatter(\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==3,\"CCS\"],\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==3,\"LSTM_predictions\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=3\"\n)\n\nplt.scatter(\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==4,\"CCS\"],\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==4,\"LSTM_predictions\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=4\"\n)\n\n# Plot a diagonal the points should be one\nplt.plot([300,1100],[300,1100],c=\"grey\")\n\nlegend = plt.legend()\n\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n\n# Get the predictions and calculate performance metrics\npredictions = ccs_df_test[\"LSTM_predictions\"]\nmare = round(sum((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100)/len(predictions),3)\npcc = round(pearsonr(predictions,ccs_df_test.loc[:,\"CCS\"])[0],3)\nperc_95 = round(np.percentile((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100,95)*2,2)\n\nplt.title(f\"LSTM - PCC: {pcc} - MARE: {mare}% - 95th percentile: {perc_95}%\")\n\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel(\"Observed CCS (^2)\")\nplt.ylabel(\"Predicted CCS (^2)\")\n\nplt.show()\n\n\n\n\nIt is clear that the performance of this model is much better. But‚Ä¶ Performance can be improved a lot more by for example tuning hyperparameters like the network architecture or number of epochs.\nHope you enjoyed this tutorial! Feel free to edit it and make a pull request!"
  },
  {
    "objectID": "tutorials/retentiontime/dlomix-prosit-rt.html",
    "href": "tutorials/retentiontime/dlomix-prosit-rt.html",
    "title": "DLOmix embedding of Prosit model on ProteomeTools data",
    "section": "",
    "text": "Open In Colab\n\n\n\n!pip install pandas sklearn tensorflow dlomix numpy matplotlib dlomix requests\n\n\n# Import and normalize/standarize data\nimport pandas as pd\nimport numpy as np\n# Import and normalize the data\ndata = pd.read_csv('https://github.com/ProteomicsML/ProteomicsML/blob/main/datasets/retentiontime/ProteomeTools/Small.csv.gz?raw=true', compression='gzip')\n\n# shuffle and split dataset into internal (80%) and external (20%) datasets\ndata = data.sample(frac=1)\ntest_data = data[int(len(data)*0.8):]\ndata = data[:int(len(data)*0.8)]\n\n\n# Split the internal dataset into training and validation\n# We have to split the data based on Sequences, to make sure we dont have cross-over sequences in the training and validation splits.\nunique_sequences = list(set(data['Sequence']))\n# Shuffle the data to ensure unbiased data splitting\nfrom random import shuffle\nshuffle(unique_sequences)\n# Split sequence 80-10-10 training, validation and testing split\ntrain = unique_sequences[0:int(len(unique_sequences) * 0.8)]\nvalidation = unique_sequences[int(len(unique_sequences) * 0.8):]\n# Transfer the sequence split into data split\ntrain = data[data['Sequence'].isin(train)]\nvalidation = data[data['Sequence'].isin(validation)]\nprint('Training data points:', len(train),'  Validation data points:',  len(validation),'  Testing data points:',  len(test_data))\n# Here we use test as an external dataset unlike the one used for training.\n\nTraining data points: 64428   Validation data points: 15572   Testing data points: 20000\n\n\n\nnormalize = True\nif normalize:\n  # Normalize\n  train_val_min, train_val_max = min(train['Retention time'].min(), validation['Retention time'].min()), max(train['Retention time'].max(), validation['Retention time'].max())\n  train['Retention time'] = list((train['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n  validation['Retention time'] = list((validation['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n  test_data['Retention time'] = list((test_data['Retention time'] - test_data['Retention time'].min()) / (test_data['Retention time'].max() - test_data['Retention time'].min()))\nelse:\n  # Standardize\n  train_val_mean, train_val_std = np.mean(list(train['Retention time']) + list(validation['Retention time'])), np.std(list(train['Retention time']) + list(validation['Retention time']))\n  train['Retention time'] = (train['Retention time'] - train_val_mean) / train_val_std\n  validation['Retention time'] = (validation['Retention time'] - train_val_mean) / train_val_std\n  test_data['Retention time'] = (test_data['Retention time'] - np.mean(test_data['Retention time'])) / np.std(test_data['Retention time'])\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n\n\n\n# Setup parameters\nsequence_length = 30\nbatch_size = 64\nepochs=10\n\n\n# Setup data \nfrom dlomix.data import RetentionTimeDataset\ntrain_input = RetentionTimeDataset(data_source=tuple([np.array(train['Sequence']), np.array(train['Retention time'])]), \n                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n\nval_input = RetentionTimeDataset(data_source=tuple([np.array(validation['Sequence']), np.array(validation['Retention time'])]), \n                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n\ntest_input = RetentionTimeDataset(data_source=tuple([np.array(test_data['Sequence']), np.array(test_data['Retention time'])]), \n                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n\n# Setup PROSIT model from DLOmix\nfrom dlomix.models.prosit import PrositRetentionTimePredictor\nmodel = PrositRetentionTimePredictor(seq_length=sequence_length)\nmodel.build((None, sequence_length))\nmodel.summary()\n\nModel: \"prosit_retention_time_predictor\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n string_lookup (StringLookup  multiple                 0         \n )                                                               \n                                                                 \n embedding (Embedding)       multiple                  352       \n                                                                 \n sequential (Sequential)     (None, 30, 512)           1996800   \n                                                                 \n attention_layer (AttentionL  multiple                 542       \n ayer)                                                           \n                                                                 \n sequential_1 (Sequential)   (None, 512)               262656    \n                                                                 \n dense_1 (Dense)             multiple                  513       \n                                                                 \n=================================================================\nTotal params: 2,260,863\nTrainable params: 2,260,863\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom dlomix.eval.rt_eval import TimeDeltaMetric\nimport tensorflow as tf\n# Compiling the keras model with loss function, metrics and optimizer\nmodel.compile(loss='mse', metrics=['mae', TimeDeltaMetric()], optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n# Train the model\nhistory = model.fit(x=train_input, epochs=epochs, batch_size=batch_size, validation_data=val_input)\n\nEpoch 1/10\n981/981 [==============================] - 32s 22ms/step - loss: 0.0204 - mae: 0.0769 - timedelta: 0.0743 - val_loss: 0.0043 - val_mae: 0.0441 - val_timedelta: 0.0484\nEpoch 2/10\n981/981 [==============================] - 21s 21ms/step - loss: 0.0037 - mae: 0.0415 - timedelta: 0.0433 - val_loss: 0.0025 - val_mae: 0.0303 - val_timedelta: 0.0330\nEpoch 3/10\n981/981 [==============================] - 21s 21ms/step - loss: 0.0034 - mae: 0.0394 - timedelta: 0.0397 - val_loss: 0.0034 - val_mae: 0.0358 - val_timedelta: 0.0395\nEpoch 4/10\n981/981 [==============================] - 21s 21ms/step - loss: 0.0031 - mae: 0.0375 - timedelta: 0.0376 - val_loss: 0.0029 - val_mae: 0.0336 - val_timedelta: 0.0352\nEpoch 5/10\n981/981 [==============================] - 21s 21ms/step - loss: 0.0030 - mae: 0.0370 - timedelta: 0.0392 - val_loss: 0.0030 - val_mae: 0.0334 - val_timedelta: 0.0387\nEpoch 6/10\n981/981 [==============================] - 20s 21ms/step - loss: 0.0030 - mae: 0.0371 - timedelta: 0.0387 - val_loss: 0.0023 - val_mae: 0.0276 - val_timedelta: 0.0296\nEpoch 7/10\n981/981 [==============================] - 20s 21ms/step - loss: 0.0029 - mae: 0.0366 - timedelta: 0.0394 - val_loss: 0.0023 - val_mae: 0.0294 - val_timedelta: 0.0307\nEpoch 8/10\n981/981 [==============================] - 20s 20ms/step - loss: 0.0029 - mae: 0.0368 - timedelta: 0.0395 - val_loss: 0.0024 - val_mae: 0.0300 - val_timedelta: 0.0311\nEpoch 9/10\n981/981 [==============================] - 20s 21ms/step - loss: 0.0028 - mae: 0.0363 - timedelta: 0.0382 - val_loss: 0.0028 - val_mae: 0.0326 - val_timedelta: 0.0335\nEpoch 10/10\n981/981 [==============================] - 20s 21ms/step - loss: 0.0027 - mae: 0.0349 - timedelta: 0.0369 - val_loss: 0.0023 - val_mae: 0.0265 - val_timedelta: 0.0290\n\n\n\nfrom dlomix.reports import RetentionTimeReport\nreport = RetentionTimeReport(output_path=\"./output\", history=history)\n\n\nreport.plot_keras_metric(\"loss\")\n\n\n\n\n\nreport.plot_keras_metric(\"timedelta\")\n\n\n\n\n\ny_real = np.concatenate([y for x, y in val_input], axis=0)\ny_pred = model.predict(validation['Sequence'][:len(y_real)])\nreport.plot_residuals(y_real, y_pred, xrange=(-1, 1))\n\n\n\n\n\nhistory = model.fit(x=test_input, epochs=epochs, batch_size=batch_size)\nimport matplotlib.pyplot as plt\nplt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\nplt.title(f'Training and validation loss of the refined model')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/retentiontime/manual-prosit-rt.html",
    "href": "tutorials/retentiontime/manual-prosit-rt.html",
    "title": "Manual embedding of Bi-LSTM model on ProteomeTools data",
    "section": "",
    "text": "Open In Colab\n\n\n\n!pip install pandas sklearn tensorflow dlomix numpy matplotlib requests\n\n\n# Import and normalize/standarize data\nimport pandas as pd\nimport numpy as np\n# Import and normalize the data\ndata = pd.read_csv('https://github.com/ProteomicsML/ProteomicsML/blob/main/datasets/retentiontime/ProteomeTools/Small.csv.gz?raw=true', compression='gzip')\n\n# shuffle and split dataset into internal (80%) and external (20%) datasets\ndata = data.sample(frac=1)\ntest_data = data[int(len(data)*0.8):]\ndata = data[:int(len(data)*0.8)]\n\n\n# Split the internal dataset into training and validation\n# We have to split the data based on Sequences, to make sure we dont have cross-over sequences in the training and validation splits.\nunique_sequences = list(set(data['Sequence']))\n# Shuffle the data to ensure unbiased data splitting\nfrom random import shuffle\nshuffle(unique_sequences)\n# Split sequence 80-10-10 training, validation and testing split\ntrain = unique_sequences[0:int(len(unique_sequences) * 0.8)]\nvalidation = unique_sequences[int(len(unique_sequences) * 0.8):]\n# Transfer the sequence split into data split\ntrain = data[data['Sequence'].isin(train)]\nvalidation = data[data['Sequence'].isin(validation)]\nprint('Training data points:', len(train),'  Validation data points:',  len(validation),'  Testing data points:',  len(test_data))\n# Here we use test as an external dataset unlike the one used for training.\n\nTraining data points: 64613   Validation data points: 15387   Testing data points: 20000\n\n\n\nnormalize = True\nif normalize:\n  # Normalize\n  train_val_min, train_val_max = min(train['Retention time'].min(), validation['Retention time'].min()), max(train['Retention time'].max(), validation['Retention time'].max())\n  train['Retention time'] = list((train['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n  validation['Retention time'] = list((validation['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n  test_data['Retention time'] = list((test_data['Retention time'] - test_data['Retention time'].min()) / (test_data['Retention time'].max() - test_data['Retention time'].min()))\nelse:\n  # Standardize\n  train_val_mean, train_val_std = np.mean(list(train['Retention time']) + list(validation['Retention time'])), np.std(list(train['Retention time']) + list(validation['Retention time']))\n  train['Retention time'] = (train['Retention time'] - train_val_mean) / train_val_std\n  validation['Retention time'] = (validation['Retention time'] - train_val_mean) / train_val_std\n  test_data['Retention time'] = (test_data['Retention time'] - np.mean(test_data['Retention time'])) / np.std(test_data['Retention time'])\n\n\n# Setup parameters\nsequence_length = 30\nbatch_size = 64\nepochs=10\n\n\n# Manual sequence embedding\n# Remove sequences longer than our maximum sequence length\ntrain = train[train['Sequence'].str.len()<=sequence_length]\nvalidation = validation[validation['Sequence'].str.len()<=sequence_length]\ntest_data = test_data[test_data['Sequence'].str.len()<=sequence_length]\n\n# Create an alphabet to convert from string to numeric\nAA_alphabet = {\"A\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"H\": 7, \"I\": 8, \"K\": 9, \"L\": 10, \"M\": 11, \"N\": 12, \"P\": 13, \"Q\": 14, \"R\": 15, \"S\": 16, \"T\": 17, \"V\": 18, \"W\": 19, \"Y\": 20}\n# Convert sequences from string to numberic\nembedded_sequences_train = [[AA_alphabet[g] for g in f] for f in train['Sequence']]\nembedded_sequences_validation = [[AA_alphabet[g] for g in f] for f in validation['Sequence']]\nembedded_sequences_test = [[AA_alphabet[g] for g in f] for f in test_data['Sequence']]\n\n# Make sure every sequence is the same length\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nembedded_sequences_train = pad_sequences(sequences=embedded_sequences_train, maxlen=sequence_length)\nembedded_sequences_validation = pad_sequences(sequences=embedded_sequences_validation, maxlen=sequence_length)\nembedded_sequences_test = pad_sequences(sequences=embedded_sequences_test, maxlen=sequence_length)\n\n\n# Import the needed layers and tensorflow model requirements\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Concatenate, Bidirectional, Dropout\nfrom tensorflow.keras.models import Model\n\ninputs = Input(shape=(sequence_length,), name='Input')\n# Embed the sequnces in a 20 x 8 matrix\ninput_embedding = Embedding(input_dim=len(AA_alphabet)+2, output_dim=8, name='Sequence_Embedding')(inputs)\nx = Bidirectional(LSTM(32, return_sequences=True), name='Bi_LSTM_1')(input_embedding)\nx = Dropout(0.25, name='LSTM_Dropout')(x)\nx = Bidirectional(LSTM(32), name='Bi_LSTM_2')(x)\noutput = Dense(1, activation=\"linear\", name='Output')(x)\nmodel = Model(inputs, output)\nmodel.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 30)]              0         \n                                                                 \n Sequence_Embedding (Embeddi  (None, 30, 8)            176       \n ng)                                                             \n                                                                 \n Bi_LSTM_1 (Bidirectional)   (None, 30, 64)            10496     \n                                                                 \n LSTM_Dropout (Dropout)      (None, 30, 64)            0         \n                                                                 \n Bi_LSTM_2 (Bidirectional)   (None, 64)                24832     \n                                                                 \n Output (Dense)              (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 35,569\nTrainable params: 35,569\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nimport tensorflow as tf\n# Compiling the keras model with loss function, metrics and optimizer\nmodel.compile(loss='mse', metrics=['mae'], optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n# Train the model\nhistory = model.fit(x=embedded_sequences_train, y=train['Retention time'], epochs=epochs, \n                    batch_size=batch_size, validation_data=(embedded_sequences_validation, validation['Retention time']))\n\nEpoch 1/10\n983/983 [==============================] - 26s 19ms/step - loss: 0.0073 - mae: 0.0556 - val_loss: 0.0036 - val_mae: 0.0406\nEpoch 2/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0035 - mae: 0.0395 - val_loss: 0.0028 - val_mae: 0.0340\nEpoch 3/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0029 - mae: 0.0355 - val_loss: 0.0034 - val_mae: 0.0361\nEpoch 4/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0026 - mae: 0.0322 - val_loss: 0.0026 - val_mae: 0.0317\nEpoch 5/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0025 - mae: 0.0313 - val_loss: 0.0024 - val_mae: 0.0280\nEpoch 6/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0023 - mae: 0.0297 - val_loss: 0.0025 - val_mae: 0.0313\nEpoch 7/10\n983/983 [==============================] - 14s 14ms/step - loss: 0.0023 - mae: 0.0292 - val_loss: 0.0024 - val_mae: 0.0277\nEpoch 8/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0022 - mae: 0.0288 - val_loss: 0.0023 - val_mae: 0.0296\nEpoch 9/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0021 - mae: 0.0281 - val_loss: 0.0021 - val_mae: 0.0271\nEpoch 10/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0021 - mae: 0.0276 - val_loss: 0.0025 - val_mae: 0.0347\n\n\n\nimport matplotlib.pyplot as plt\n# Plotting the training history \nplt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\nplt.plot(range(epochs), history.history['val_loss'], '--', color='r', label='Validation loss')\nplt.title(f'Training and validation loss across epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Initially we trained on just one gradient, to transfer this model to external datasets, \n# we refine the model by using the model we just trained as a pre-trained model, and then further train it with the test/external dataset\nhistory = model.fit(x=embedded_sequences_test, y=test_data['Retention time'], epochs=epochs, batch_size=batch_size)\n# The model can now be used for other datasets with the same gradient set-up\n# We then plot the history of this model, and see the initial performance is much better, \n# as the model already has some gradient agnostic knowledge, and it simply has to learn the new gradients\nplt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\nplt.title(f'Training and validation loss of the refined model')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nEpoch 1/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0021 - mae: 0.0285\nEpoch 2/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0020 - mae: 0.0278\nEpoch 3/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0019 - mae: 0.0270\nEpoch 4/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0018 - mae: 0.0267\nEpoch 5/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0018 - mae: 0.0267\nEpoch 6/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0018 - mae: 0.0264\nEpoch 7/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0017 - mae: 0.0261\nEpoch 8/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0018 - mae: 0.0267\nEpoch 9/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0017 - mae: 0.0253\nEpoch 10/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0017 - mae: 0.0262\n\n\n\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/retentiontime/index.html",
    "href": "tutorials/retentiontime/index.html",
    "title": "Retention time",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/index.html",
    "href": "tutorials/fragmentation/index.html",
    "title": "Fragmentation",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/nist-1-parsing-spectral-library.html",
    "href": "tutorials/fragmentation/nist-1-parsing-spectral-library.html",
    "title": "NIST (part 1): Parsing the spectral library",
    "section": "",
    "text": "In bottom-up proteomics, a peptide fragmentation spectrum (MS2) is the most central source of information to identify a peptide (and the original protein by extend). In traditional peptide fragment spectrum identification workflows, only the presence and location (x-axis) of peaks in the spectrum are used to identify the peptide that generated the spectrum. The intensity of these peaks (y-axis) are, however, seldomly used in a comprehensive manner. At most, traditional approaches naively assume that higher intensity is always better. The lack of usage of peptide spectrum intensity patterns in the identification step is mainly due to their complexity. The location of certain peaks (e.g., b- and y-ions) are easily calculated based on their mass for any given peptide. Their intensity, however, follows complex yet predictable patterns that cannot be simply calculated. Nevertheless, they can be predicted with machine learning.\nIn this tutorial you will learn the basic steps in developing a machine learning predictor for peptide fragmentation intensity prediction. The first chapter handles the preparation and parsing of training data and the second chapter first handles training a traditional machine learning model, and then a deep learning model.\nTo avoid an overly complex tutorial, some aspects to intensity prediction are simplified or not handled. For example, the resulting models will only be able to predict singly charged b- and y-ions for unmodified peptides.\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#finding-spectral-libraries",
    "href": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#finding-spectral-libraries",
    "title": "NIST (part 1): Parsing the spectral library",
    "section": "1.1 Finding spectral libraries",
    "text": "1.1 Finding spectral libraries\nTraining data for peptide fragmentation spectrum intensity prediction consists of spectra that were already identified. The most convenient source of such information are spectral libraries. These are datasets that were compiled from a collection of mass spectrometry runs and usually consist of a single representative spectrum for each peptide that was identified.\nMany precompiled spectral libraries are available online. You can also generate your own from a collection of proteomics experiments, using software such as SpectraST.\nSpectral libraries can be downloaded, for instance, from NIST, the US National Institute of Standards and Technology . For this part of the practical, we will download the 2020 Human HCD library of ‚Äúbest‚Äù tryptic spectra. For ease-of-use, we will download it in the text-based NIST MSP format.\nThe following code cell automatically downloads and extracts the spectral library file using Linux commands. If you are working locally on a Windows machine, you can download and extract the files manually.\nNote the exclamation marks in the code cell below, which allow us to run Linux commands, instead of Python.\n\n# THIS CELL WORKS ONLY ON LINUX HOSTS (e.g. Google Colab)\n\n# `wget` is a Linux command line tool to download files. \n! wget -q https://chemdata.nist.gov/download/peptide_library/libraries/human/HCD/2020_05_19/human_hcd_tryp_best.msp.tar.gz\n\n# Next, we need to unpack the .tar.gz file\n! tar -xf human_hcd_tryp_best.msp.tar.gz\n\nLet‚Äôs explore the MSP spectral library file, using the Linux head command:\n\n! head human_hcd_tryp_best.msp\n\nThis shows the beginning of the first spectrum in the spectral library. Each spectrum entry consists of a header with identification data and metadata, and a peak list with three columns:\n\nm/z values\nintensity values\npeak annotation info\n\nAs the sequence of the first peptide is AAAAAAAAAAAAAAAGAGAGAK, we can assume that this library is ordered alphabetically. You can read through the file to verify this assumption. This ordering will be important later on."
  },
  {
    "objectID": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#parsing-the-msp-spectral-library-file",
    "href": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#parsing-the-msp-spectral-library-file",
    "title": "NIST (part 1): Parsing the spectral library",
    "section": "1.2 Parsing the MSP spectral library file",
    "text": "1.2 Parsing the MSP spectral library file\nPyteomics is a Python package for proteomics that contains readers for many proteomics-related file formats. Unfortunately, MSP is not one of them. So first, we need a custom MSP reader function.\n\nfrom rich import print, progress  # Rich is a pretty cool library. Google it ;)\nimport numpy as np\nimport pandas as pd\n\nThis function iterates over each line in the MSP file. Once it has gathered all information for a single spectrum, it uses yield to return a dictionary. This means that we can iterate over the function using a for loop, and process spectra one-by-one.\nIf you do not fully understand the function, no problem! This is not the important part of the tutorial üòâ\n\ndef read_msp(filename):\n    \"\"\"Iterate over MSP spectral library file and return spectra as dicts.\"\"\"\n    spectrum = {}\n    mz = []\n    intensity = []\n    annotation = []\n\n    for line in open(filename, \"rt\"):\n        # `Name: ` is the first line of a new entry in the file\n        if line.startswith(\"Name: \"):\n            if spectrum:\n                # Finalize and yield previous spectrum\n                spectrum[\"sequence\"] = spectrum[\"Fullname\"].split(\".\")[1]  # Remove the previous/next amino acids\n                spectrum[\"mz\"] = np.array(mz, dtype=\"float32\")\n                spectrum[\"intensity\"] = np.array(intensity, dtype=\"float32\")\n                spectrum[\"annotation\"] = np.array(annotation, dtype=\"str\")\n                yield spectrum\n\n                # Define new spectrum\n                spectrum = {}\n                mz = []\n                intensity = []\n                annotation = []\n            \n            # Extract everything after `Name: `\n            spectrum[\"Name\"] = line.strip()[6:]\n\n        elif line.startswith(\"Comment: \"):\n            # Parse all comment items as metadata\n            metadata = [i.split(\"=\") for i in line[9:].split(\" \")]\n            for item in metadata:\n                if len(item) == 2:\n                    spectrum[item[0]] = item[1]\n\n        elif line.startswith(\"Num peaks: \"):\n            spectrum[\"Num peaks\"] = int(line.strip()[11:])\n\n        elif len(line.split(\"\\t\")) == 3:\n            # Parse peak list items one-by-one\n            line = line.strip().split(\"\\t\")\n            mz.append(line[0])\n            intensity.append(line[1])\n            annotation.append(line[2].strip('\"'))\n\n    # Final spectrum\n    spectrum[\"sequence\"] = spectrum[\"Fullname\"].split(\".\")[1]  # Remove the previous/next amino acids\n    spectrum[\"mz\"] = np.array(mz, dtype=\"float32\")\n    spectrum[\"intensity\"] = np.array(intensity, dtype=\"float32\")\n    spectrum[\"annotation\"] = np.array(annotation, dtype=\"str\")\n    yield spectrum\n\nLet‚Äôs explore the first spectrum:\n\n# break allows us to only stop after the first spectrum is defined\nfor spectrum in read_msp(\"human_hcd_tryp_best.msp\"):\n    print(spectrum[\"Name\"])\n    break\n\nWe can format the peak list as a Pandas DataFrame:\n\npd.DataFrame({\n    \"mz\": spectrum[\"mz\"],\n    \"intensity\": spectrum[\"intensity\"],\n    \"annotation\": spectrum[\"annotation\"],\n})\n\nThe left-most column denotes the peak annotation. This tells us which ion generated the peak, according to the search engine or library generation software. Note that many peaks ‚Äî highlighted with a question mark ‚Äî are not annotated, even though the spectrum was confidently identified.\nUsing the Python package spectrum_utils, we can easily visualize the spectrum:\n\nimport matplotlib.pyplot as plt\n\nimport spectrum_utils.spectrum as sus\nimport spectrum_utils.plot as sup\n\n\nplt.figure(figsize=(12,6))\nsup.spectrum(\n    sus.MsmsSpectrum(\n        identifier=spectrum[\"Name\"],\n        precursor_mz=float(spectrum[\"Parent\"]),\n        precursor_charge=int(spectrum[\"Charge\"]),\n        mz=spectrum[\"mz\"],\n        intensity=spectrum[\"intensity\"]\n    )\n)\nplt.show()"
  },
  {
    "objectID": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#preparing-spectra-for-training",
    "href": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#preparing-spectra-for-training",
    "title": "NIST (part 1): Parsing the spectral library",
    "section": "1.3 Preparing spectra for training",
    "text": "1.3 Preparing spectra for training\nTo use a peptide fragmentation spectrum such as this one as training target for a machine learning model, it needs some preparation and parsing. Usually this comprises of the following steps:\n\nNormalize the intensities\nTransform the intensities\nAnnotate the peaks\nParse the relevant peak intensities to an format suitable for machine learning\n\nFor each of these steps, we will write a function that can be reused later on in the tutorial.\n\n1.3.1 Normalize the intensities\nDepending on the file format, peak intensities can range from 0 to 1, from 0 to 100, from 0 from 10 000‚Ä¶ Machine learning algorithms require the target (and feature) values to be normalized in a specific range. For fragmentation spectra, there are two common options: total ion current (TIC) normalization and base peak normalization. For the former, all intensity values are divided by the total sum of all intensity values in the spectrum. The sum of all normalized intensities will be 1. For the latter, all intensity values are divided by the most intense peak in the spectrum, resulting in that peak to have normalized intensity 1. Here we will implement TIC-normalization.\n\ndef tic_normalize(msp_spectrum):\n    tic = np.sum(msp_spectrum[\"intensity\"])\n    msp_spectrum[\"intensity\"] = msp_spectrum[\"intensity\"] / tic\n    return msp_spectrum\n\n\n# Before normalization\nspectrum[\"intensity\"][:10]\n\n\nspectrum = tic_normalize(spectrum)\n\n# After normalization\nspectrum[\"intensity\"][:10]\n\n\n\n1.3.2 Transform the intensities\nThe distribution of peak intensities shows us that most peptide fragmentation peaks have a relatively low intensity, while only a few peaks are more intense:\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n# Before transform\nsns.displot(spectrum[\"intensity\"], bins=20)\nplt.show()\n\nTo make the intensities follow a more linear distribution ‚Äî which is better for machine learning algorithms ‚Äî we can transform the intensity values. Two methods are often used: square root-tranform, and log-transform. While both methods mostly have the same effect, we will here opt for square root transform, as log-transform results in negative values, which can be cumbersome to deal with.\n\ndef sqrt_transform(msp_spectrum):\n    msp_spectrum[\"intensity\"] = np.sqrt(msp_spectrum[\"intensity\"])\n    return msp_spectrum\n\n\nspectrum = sqrt_transform(spectrum)\n\n# After transform\nsns.displot(spectrum[\"intensity\"], bins=20)\nplt.show()\n\n\n\n1.3.3 Annotate the peaks\nWith the NIST spectral libraries, this step is pretty easy, as peak annotations are already present. If this would not be the case, we can make use of spectrum_utils, which can annotate peaks given the peptide sequence and any modifications. See the spectrum_utils documentation for more info.\n\nplt.figure(figsize=(12,6))\nsup.spectrum(\n    sus.MsmsSpectrum(\n        identifier=spectrum[\"Name\"],\n        precursor_mz=float(spectrum[\"Parent\"]),\n        precursor_charge=int(spectrum[\"Charge\"]),\n        mz=spectrum[\"mz\"],\n        intensity=spectrum[\"intensity\"],\n        peptide=spectrum[\"sequence\"],\n    ).annotate_peptide_fragments(25, \"ppm\")\n)\nplt.show()\n\n\n\n1.3.4 Parse the relevant peak intensities to an format suitable for machine learning\nNote in the visualization above that spectrum_utils only annotated b- and y-ions, while in the MSP file many other ion types are also annotated. For simplicity‚Äôs sake, in this tutorial we will train a model to only predict singly charged b- and y-ions.\nLet‚Äôs filter the spectrum for only those peaks. This can be done with regular expressions (regex) and numpy. TIP: regex101.com is a great website for building and testing regular expressions. The regex ^(b|y)([0-9]+)\\/ only matches peak annotations for singly charged b- and y-ions. You can investigate it at https://regex101.com/r/bgZ7EG/1.\nIn the following function, numpy.vectorize is used. What does it do and why do we use it here?\n\nimport re\n\ndef filter_peaks(msp_spectrum):\n    \"\"\"Filter spectrum peaks to only charge 1 b- and y ions.\"\"\"\n    # Generate the boolean mask\n    get_mask = np.vectorize(lambda x: bool(re.match(\"^(b|y)([0-9]+)\\/\", x)))\n    mask = get_mask(msp_spectrum[\"annotation\"])\n    \n    # Apply the mask to each peak array\n    msp_spectrum[\"annotation\"] = msp_spectrum[\"annotation\"][mask]\n    msp_spectrum[\"mz\"] = msp_spectrum[\"mz\"][mask]\n    msp_spectrum[\"intensity\"] = msp_spectrum[\"intensity\"][mask]\n\n    return msp_spectrum\n\nspectrum = filter_peaks(spectrum)\n\n\nplt.figure(figsize=(12,6))\nsup.spectrum(\n    sus.MsmsSpectrum(\n        identifier=spectrum[\"Name\"],\n        precursor_mz=float(spectrum[\"Parent\"]),\n        precursor_charge=int(spectrum[\"Charge\"]),\n        mz=spectrum[\"mz\"],\n        intensity=spectrum[\"intensity\"],\n        peptide=spectrum[\"sequence\"]\n    ).annotate_peptide_fragments(25, \"ppm\")\n)\nplt.show()\n\nNow, the spectrum indeed only contains singly charged b- and y-ions. Note the nice gausian-like distributions of equally-distanced b- and y-ions. This is a feature specific for this peptide spectrum. Can you guess why? Tip: Take a look at the peptide sequence.\nCurrently, all peaks are listed together in single numpy arrays, sorted by m/z values. For training a machine learning model, we need the intensity values in a more suitable structure. As we are planning to only predict simple singly charged b- and y-ions, we can create two arrays ‚Äî one for each ion type ‚Äî with the ions sorted by ion number:\nparsed_intensity = {\n    \"b\": [b1, b2, b3, b4 ... bN],\n    \"y\": [y1, y2, y3, y4 ... yN]\n}\nwhere N is the total number of possible fragments for that peptide sequence. Quick question: What value will N have for our peptide with sequence AAAAAAAAAAAAAAAGAGAGAK?\nThe following function builds upon the filter_peaks function to not only filter the correct ion types, but also order them properly:\n\ndef parse_peaks(msp_spectrum, ion_type):\n    # Generate vectorized functions\n    get_ions = np.vectorize(lambda x: bool(re.match(f\"^({ion_type})([0-9]+)\\/\", x)))\n    get_ion_order = np.vectorize(lambda x: re.match(f\"^({ion_type})([0-9]+)\\/\", x)[2])\n\n    # Get mask with requested ion types\n    mask = get_ions(msp_spectrum[\"annotation\"])\n\n    # Create empty array with for all possible ions\n    n_ions = len(msp_spectrum[\"sequence\"]) - 1\n    parsed_intensity = np.zeros(n_ions)\n\n    # Check if any ions of this type are present\n    if mask.any():\n        # Filter for ion type and sort\n        ion_order = get_ion_order(msp_spectrum[\"annotation\"][mask]).astype(int) - 1\n        # Add ions to correct positions in new array\n        parsed_intensity[ion_order] = msp_spectrum[\"intensity\"][mask]\n\n    try:\n        msp_spectrum[\"parsed_intensity\"][ion_type] = parsed_intensity\n    except KeyError:\n        msp_spectrum[\"parsed_intensity\"] = {}\n        msp_spectrum[\"parsed_intensity\"][ion_type] = parsed_intensity\n    \n    return msp_spectrum\n\nspectrum = parse_peaks(spectrum, \"b\")\nspectrum = parse_peaks(spectrum, \"y\")\n\n\nspectrum['parsed_intensity']\n\nGreat! These values are now ready to be used as prediction targets for a machine learning algorithm."
  },
  {
    "objectID": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#parse-the-full-spectral-library",
    "href": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#parse-the-full-spectral-library",
    "title": "NIST (part 1): Parsing the spectral library",
    "section": "1.4 Parse the full spectral library",
    "text": "1.4 Parse the full spectral library\nNow that all functions for spectrum preparation are written, we can parse the full spectral library. Let‚Äôs first explore some of the basic statistics of this library.\n\n1.4.1 Exploring basic spectral library statistics\n\nTotal number of spectra\nThis one is easy, we quickly scan the number of lines starting with Name: in the file:\n\ndef msp_count_spectra(filename):\n    \"\"\"Count number of spectra in MSP file.\"\"\"\n    count = 0\n    for line in open(filename, \"rt\"):\n        if line.startswith(\"Name: \"):\n            count += 1\n    return count\n\nn_spectra = msp_count_spectra(\"human_hcd_tryp_best.msp\")\nprint(n_spectra)\n\nFor more statistics, we will have to read the full library. One advantage we have now, is that knowing the total number of spectra allows us to track the progress of reading the full MSP file (progress.track). To limit the amount of data we keep in memory (this full MSP file is almost 2GB!), we can process the intensity values of each spectrum while parsing and only keep the parsed data:\n\nspectrum_list = []\nfor msp_spectrum in progress.track(\n    read_msp(\"human_hcd_tryp_best.msp\"),\n    total=n_spectra,\n    description=\"Parsing  MSP file...\"\n):\n    # Process intensities\n    msp_spectrum = tic_normalize(msp_spectrum)\n    msp_spectrum = sqrt_transform(msp_spectrum)\n    msp_spectrum = parse_peaks(msp_spectrum, \"b\")  # Adds `parsed_intensity` > `b`\n    msp_spectrum = parse_peaks(msp_spectrum, \"y\")  # Adds `parsed_intensity` > `y`\n\n    # Parse metadata\n    spectrum = {\n        \"sequence\": msp_spectrum[\"sequence\"],\n        \"modifications\": msp_spectrum[\"Mods\"],\n        \"charge\": int(msp_spectrum[\"Charge\"]),\n        \"nce\": float(msp_spectrum[\"NCE\"]),\n        \"parsed_intensity\": msp_spectrum[\"parsed_intensity\"]\n    }\n\n    # Append to list\n    spectrum_list.append(spectrum)\n\nGenerating a Pandas DataFrame from the list of spectrum dictionaries, allows us to easily explore the full dataset:\n\nspectrum_df = pd.DataFrame(spectrum_list)\nspectrum_df\n\nMaking a Pandas DataFrame out of spectrum_list is so simple because it is a list of consistent dictionaries.\n\n\nPrecursor charge state\nA different precursor charge state can heavily alter peptide fragmentation. It is therefore important to have a representative amount of peptide spectra for each charge state in the spectral library.\n\nsns.countplot(data=spectrum_df, x=\"charge\")\nplt.show()\n\n\n\nPeptide length\nIdem for the length of the peptide sequence. It usually makes sense to filter the train dataset for peptides within a certain length range.\n\nsns.kdeplot(spectrum_df[\"sequence\"].str.len())\nplt.xlabel(\"Sequence length\")\nplt.show()\n\n\nspectrum_df[\"sequence\"].str.len().describe()\n\n\n(spectrum_df[\"sequence\"].str.len() > 35).value_counts()\n\nFor this dataset, the minimum peptide length is 6, while the maximum is 50. Nevertheless, only\n\nPeptide modifications\nLikewise, peptide modifications can influence peptide fragmentation. How many of the spectra in our library come from modified peptides?\n\nmodification_state = (spectrum_df[\"modifications\"] == \"0\").map({True: \"Unmodified\", False: \"Modified\"})\nsns.countplot(x=modification_state)\nplt.show()\n\n\n\n\nCollision energy\n\nsns.histplot(spectrum_df[\"nce\"], bins=30)\nplt.xlabel(\"NCE\")\nplt.show()\n\nNote the range of the x-axis, which was automatically chosen by the plotting library. It seems to start at 0, which indicates that some values are very low‚Ä¶\n\n(spectrum_df[\"nce\"] == 0.0).value_counts()\n\nIndeed, it seems that some peptide spectra have collision energy (CE) 0, which most likely means that the true collision energy setting is not known. We can either opt to not use CE as a feature for training, or to remove these spectra from the dataset. Including these values would introduce unwanted noise in the training data.\n\n\nDuplicate entries?\nAn important aspect to compiling training data for machine learning is whether or not entries are duplicated. With spectral libraries, matters are complicated by multiple levels of ‚Äúuniqueness‚Äù:\n\nPeptide level: Unique sequence\nPeptidoform level: Unique sequence & modifications\nPrecursor level: Unique sequence & modifications & charge\n\nMore parameters can be included for ‚Äúuniqueness‚Äù, such as instrument and acquisition properties: CE, fragmentation method (beam-type CID (‚ÄúHCD‚Äù), trap-type CID, ETD, EAD‚Ä¶), acquisition method (Orbitrap, ion trap, TOF‚Ä¶). In this tutorial, we are using only HCD Orbitrap data, which makes things a bit simpler. Nevertheless, this will impact the application domain of the final models.\n\ncounts = pd.DataFrame({\n    \"Level\": [\n        \"Full library\",\n        \"Precursor\",\n        \"Peptidoform\",\n        \"Peptide\",\n    ],\n    \"Count\": [\n        spectrum_df.shape[0],\n        spectrum_df[[\"sequence\", \"modifications\", \"charge\"]].drop_duplicates().shape[0],\n        spectrum_df[[\"sequence\", \"modifications\"]].drop_duplicates().shape[0],\n        spectrum_df[\"sequence\"].unique().shape[0],\n    ],\n})\n\n\nsns.barplot(data=counts, x=\"Level\", y=\"Count\")\nplt.show()\n\n\ncounts\n\nSeems like this library was filtered for uniqueness on the precursor level.\n\n\n\n1.4.1 Selecting data\nFor selecting training data, we will apply some additional filters:\n\nWhile plain amino acid sequences are straightforward to encode, peptide modifications complicate matters. For simplicity‚Äôs sake, we will therefore not open the ‚Äúcan of modifications‚Äù in this tutorial.\nAs we might want to use CE as a feature, we can remove the small amount of entries that are missing the a CE value\nTo make the training task a bit less complex, we can limit peptide length to 35. Although the maximum peptide length in this library is 50, only 4944 spectra have a peptide length of over 35.\n\n\nspectrum_df = spectrum_df[\n    (modification_state == \"Unmodified\") &\n    (spectrum_df[\"sequence\"].str.len() <= 35) &\n    (spectrum_df[\"nce\"] != 0)\n]\n\nLet‚Äôs see how many spectra we retained:\n\nspectrum_df.shape[0]\n\n\nspectrum\n\n\n\n1.4.2 Train / validation / test split\nNow that we have our data, we can filter it to a final set for training and validation and a final set for testing. A small reminder of what these terms mean:\n\nTraining data: For training the model\nValidation data: For validating the model while optimizing hyperparameters\nTesting data: For final testing of model that was trained with the best hyperparameters (according to the validation data), right before deployment\n\nThe testing data cannot be used until a final model is trained, and serves as a last test before deployment. It should not be used before a final model is selected.\n\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed()  # Why is this needed?\n\ntrain_val_peptides, test_peptides = train_test_split(spectrum_df[\"sequence\"].unique(), train_size=0.9)\ntrain_val_spectra = spectrum_df[spectrum_df[\"sequence\"].isin(train_val_peptides)]\ntest_spectra = spectrum_df[spectrum_df[\"sequence\"].isin(test_peptides)]\n\nQuestion: Why do we not apply train_test_split() directly on spectrum_df, but instead on spectrum_df[\"sequence\"].unique()?"
  },
  {
    "objectID": "tutorials/fragmentation/raw-to-prosit.html",
    "href": "tutorials/fragmentation/raw-to-prosit.html",
    "title": "Raw file processing with PROSIT style annotation",
    "section": "",
    "text": "Open In Colab\n\n\nThis notebook contains the simplest steps to turn any raw data into a format thats fragmentation prediction ready. This notebook retrieve a ProteomeTools file from PRIDE to make it as easy to copy as possible, but retrieving the files might take time.\nThis method uses the MaxQuant file to get the modified sequence, charge, and scan number. It then uses fisher_py to interact with the raw files and retrieve the ms2 scans and the mass analyzer.\nThe annotation pipeline comes from the TUM annotation github\n\n%%capture\n# We need conda to interact with the raw file\n!pip install -q condacolab\nimport condacolab\ncondacolab.install()\n!conda install pythonnet\n!pip install fisher_py\n!pip install fundamentals@git+https://github.com/wilhelm-lab/spectrum_fundamentals@proteomicsml\n\n\n!wget https://ftp.pride.ebi.ac.uk/pride/data/archive/2017/02/PXD004732/01625b_GA1-TUM_first_pool_1_01_01-DDA-1h-R2.raw \n!wget https://ftp.pride.ebi.ac.uk/pride/data/archive/2017/02/PXD004732/TUM_first_pool_1_01_01_DDA-1h-R2-tryptic.zip\n\n\nfrom zipfile import ZipFile\nimport pandas as pd\nwith ZipFile(f'TUM_first_pool_1_01_01_DDA-1h-R2-tryptic.zip', 'r') as zip_file:\n  msms = pd.read_csv(zip_file.open('msms.txt'), sep='\\t')\n# Current PROSIT pipeline does not accomodate modified peptides, so we remove all of the oxidized peptides\nmsms = msms[msms['Modifications'] == 'Unmodified']\n\n\nfrom fisher_py import RawFile\nraw = RawFile('01625b_GA1-TUM_first_pool_1_01_01-DDA-1h-R2.raw')\n# Get the scan numbers from the msms file and save the scan + info in a dictionary\nfrom fisher_py.data.business import Scan\nimport numpy as np\nscan_mzs = []\nscan_ints = []\nscan_mass_analyzers = []\nscan_collison_energy = []\nfor scan in msms['Scan number']:\n  raw_scan = Scan.from_file(raw._raw_file_access, scan)\n  scan_mzs.append(np.array(raw_scan.preferred_masses))\n  scan_ints.append(np.array(raw_scan.preferred_intensities))\n  scan_mass_analyzers.append(raw_scan.scan_type.split(' + ')[0])\n  frag_infos = [f.split(' ')[0] for f in raw_scan.scan_type.split('@')[1:]]\n  splits = [[i for i, g in enumerate(f) if g.isnumeric()][0] for f in frag_infos]\n  NCEs = [float(frag[split:]) for split, frag in zip(splits, frag_infos)]\n  scan_collison_energy.append(NCEs[0])\n\nWe need to create a sub-set of the MaxQuant dataframe that we can insert into the annotation pipeline. For this we need the have 6 columns (with specific names): MODIFIED_SEQUENCE, PERCURSOR_CHARGE, MASS_ANALYZER, SCAN_NUMBER, MZ, INTENSITIES\n\nannotation_df = pd.DataFrame(msms[['Modified sequence', 'Charge', 'Scan number']].values, columns=['MODIFIED_SEQUENCE', 'PRECURSOR_CHARGE', 'SCAN_NUMBER'])\nannotation_df['MZ'] = scan_mzs\nannotation_df['INTENSITIES'] = scan_ints\nannotation_df['MASS_ANALYZER'] = scan_mass_analyzers\nannotation_df['COLLISION_ENERGY'] = scan_collison_energy\n\nfrom fundamentals.mod_string import maxquant_to_internal\nannotation_df['MODIFIED_SEQUENCE'] = maxquant_to_internal(annotation_df['MODIFIED_SEQUENCE'].values)\n\nfrom fundamentals.annotation.annotation import annotate_spectra\nannotation = annotate_spectra(annotation_df)\n\n2022-09-20 11:42:00,063 - INFO - fundamentals.annotation.annotation::annotate_spectra Removed count    13089.000000\nmean         0.007640\nstd          0.090518\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%          0.000000\nmax          2.000000\nName: removed_peaks, dtype: float64 redundant peaks\n\n\nThe annotation element contains the annotated intensities nad m/zs, along with the theoretical mass and removed peaks\n\nannotation \n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      INTENSITIES\n      MZ\n      CALCULATED_MASS\n      removed_peaks\n    \n  \n  \n    \n      0\n      [0.36918813165578857, 0.0, -1.0, 0.0, 0.0, -1....\n      [175.11929321289062, 0.0, -1.0, 0.0, 0.0, -1.0...\n      796.423175\n      0\n    \n    \n      1\n      [0.028514689782729, 0.0, -1.0, 0.0, 0.0, -1.0,...\n      [175.25360107421875, 0.0, -1.0, 0.0, 0.0, -1.0...\n      796.423175\n      0\n    \n    \n      2\n      [0.3452339640378655, 0.0, -1.0, 0.0, 0.0, -1.0...\n      [175.11927795410156, 0.0, -1.0, 0.0, 0.0, -1.0...\n      796.423175\n      0\n    \n    \n      3\n      [0.030064791591335877, 0.0, -1.0, 0.0, 0.0, -1...\n      [175.16168212890625, 0.0, -1.0, 0.0, 0.0, -1.0...\n      796.423175\n      0\n    \n    \n      4\n      [0.0, 0.0, -1.0, 0.0, 0.0, -1.0, 0.07584115481...\n      [0.0, 0.0, -1.0, 0.0, 0.0, -1.0, 262.248901367...\n      1370.559481\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      13084\n      [0.009784486409648692, 0.0, -1.0, 0.0, 0.0, -1...\n      [147.1424102783203, 0.0, -1.0, 0.0, 0.0, -1.0,...\n      914.474935\n      0\n    \n    \n      13085\n      [0.23857646569260368, 0.0, -1.0, 0.0, 0.0, -1....\n      [147.11309814453125, 0.0, -1.0, 0.0, 0.0, -1.0...\n      914.474935\n      0\n    \n    \n      13086\n      [0.012048242613237779, 0.0, -1.0, 0.0, 0.0, -1...\n      [147.1204376220703, 0.0, -1.0, 0.0, 0.0, -1.0,...\n      914.474935\n      0\n    \n    \n      13087\n      [0.39071905153057307, 0.0, -1.0, 0.0, 0.0, -1....\n      [147.11328125, 0.0, -1.0, 0.0, 0.0, -1.0, 276....\n      914.474935\n      0\n    \n    \n      13088\n      [0.02029996314040732, 0.0, -1.0, 0.0, 0.0, -1....\n      [147.19485473632812, 0.0, -1.0, 0.0, 0.0, -1.0...\n      914.474935\n      0\n    \n  \n\n13089 rows √ó 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNow we need to combined the necessary information from MaxQuant and the annotation package into a DataFrame mimicing the one found in the ‚ÄúProsit-style GRU with ProteomeTools data‚Äù found here (https://www.proteomicsml.org/tutorials/fragmentation/proteometools-prosit.html) for an easy handover\n\nPROSIT_ALHABET = {\n    \"A\": 1,\n    \"C\": 2,\n    \"D\": 3,\n    \"E\": 4,\n    \"F\": 5,\n    \"G\": 6,\n    \"H\": 7,\n    \"I\": 8,\n    \"K\": 9,\n    \"L\": 10,\n    \"M\": 11,\n    \"N\": 12,\n    \"P\": 13,\n    \"Q\": 14,\n    \"R\": 15,\n    \"S\": 16,\n    \"T\": 17,\n    \"V\": 18,\n    \"W\": 19,\n    \"Y\": 20,\n    \"M(ox)\": 21,\n}\nsequence_integer = [[PROSIT_ALHABET[AA] for AA in sequence] for sequence in msms['Sequence']]\nprecursor_charge_onehot = pd.get_dummies(msms['Charge']).values\ncollision_energy_aligned_normed = annotation_df['COLLISION_ENERGY']\nintensities_raw = annotation['INTENSITIES']\n\n\ndf = pd.DataFrame(list(zip(sequence_integer, precursor_charge_onehot, collision_energy_aligned_normed, intensities_raw)), \n                  columns=['sequence_integer', 'precursor_charge_onehot', 'collision_energy', 'intensities_raw'])\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      sequence_integer\n      precursor_charge_onehot\n      collision_energy\n      intensities_raw\n    \n  \n  \n    \n      0\n      [1, 1, 1, 5, 20, 18, 15]\n      [0, 1, 0]\n      28.0\n      [0.36918813165578857, 0.0, -1.0, 0.0, 0.0, -1....\n    \n    \n      1\n      [1, 1, 1, 5, 20, 18, 15]\n      [0, 1, 0]\n      35.0\n      [0.028514689782729, 0.0, -1.0, 0.0, 0.0, -1.0,...\n    \n    \n      2\n      [1, 1, 1, 5, 20, 18, 15]\n      [0, 1, 0]\n      28.0\n      [0.3452339640378655, 0.0, -1.0, 0.0, 0.0, -1.0...\n    \n    \n      3\n      [1, 1, 1, 5, 20, 18, 15]\n      [0, 1, 0]\n      35.0\n      [0.030064791591335877, 0.0, -1.0, 0.0, 0.0, -1...\n    \n    \n      4\n      [1, 1, 5, 17, 4, 2, 2, 14, 1, 1, 3, 9]\n      [0, 1, 0]\n      35.0\n      [0.0, 0.0, -1.0, 0.0, 0.0, -1.0, 0.07584115481...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      13084\n      [20, 20, 16, 8, 10, 4, 9]\n      [0, 1, 0]\n      35.0\n      [0.009784486409648692, 0.0, -1.0, 0.0, 0.0, -1...\n    \n    \n      13085\n      [20, 20, 16, 8, 10, 4, 9]\n      [0, 1, 0]\n      28.0\n      [0.23857646569260368, 0.0, -1.0, 0.0, 0.0, -1....\n    \n    \n      13086\n      [20, 20, 16, 8, 10, 4, 9]\n      [0, 1, 0]\n      35.0\n      [0.012048242613237779, 0.0, -1.0, 0.0, 0.0, -1...\n    \n    \n      13087\n      [20, 20, 16, 8, 10, 4, 9]\n      [0, 1, 0]\n      28.0\n      [0.39071905153057307, 0.0, -1.0, 0.0, 0.0, -1....\n    \n    \n      13088\n      [20, 20, 16, 8, 10, 4, 9]\n      [0, 1, 0]\n      35.0\n      [0.02029996314040732, 0.0, -1.0, 0.0, 0.0, -1....\n    \n  \n\n13089 rows √ó 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/manual-prosit-int.html",
    "href": "tutorials/fragmentation/manual-prosit-int.html",
    "title": "Prosit-style GRU with pre-annotated ProteomeTools data",
    "section": "",
    "text": "Open In Colab\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/manual-prosit-int.html#download-and-prepare-training-data",
    "href": "tutorials/fragmentation/manual-prosit-int.html#download-and-prepare-training-data",
    "title": "Prosit-style GRU with pre-annotated ProteomeTools data",
    "section": "Download and prepare training data",
    "text": "Download and prepare training data\n\n# Load ProteomeTools data from Figshare\n!wget https://figshare.com/ndownloader/files/12506534 \n!mv 12506534 prosit_2018_holdout.hdf5\n\n\n# Import packages\nimport pandas as pd\nimport numpy as np\nimport h5py as h5\n\n# Using the alphabet as defined in Prosit: \n# https://github.com/kusterlab/prosit/blob/master/prosit/constants.py#L21-L43\nPROSIT_ALHABET = {\n    \"A\": 1,\n    \"C\": 2,\n    \"D\": 3,\n    \"E\": 4,\n    \"F\": 5,\n    \"G\": 6,\n    \"H\": 7,\n    \"I\": 8,\n    \"K\": 9,\n    \"L\": 10,\n    \"M\": 11,\n    \"N\": 12,\n    \"P\": 13,\n    \"Q\": 14,\n    \"R\": 15,\n    \"S\": 16,\n    \"T\": 17,\n    \"V\": 18,\n    \"W\": 19,\n    \"Y\": 20,\n    \"M(ox)\": 21,\n}\nPROSIT_INDEXED_ALPHABET = {i: c for c, i in PROSIT_ALHABET.items()}\n\n\n# Read the downloaded data to a dataframe\nwith h5.File('prosit_2018_holdout.hdf5', 'r') as f:\n  KEY_ARRAY = [\"sequence_integer\", \"precursor_charge_onehot\", \"intensities_raw\"]\n  KEY_SCALAR = [\"collision_energy_aligned_normed\", \"collision_energy\"]\n  df = pd.DataFrame({key: list(f[key][...]) for key in KEY_ARRAY})\n  for key in KEY_SCALAR:\n    df[key] = f[key][...]\n\n# Add convenience columns\ndf['precursor_charge'] = df.precursor_charge_onehot.map(lambda a: a.argmax() + 1)\ndf['sequence_maxquant'] = df.sequence_integer.map(lambda s: \"\".join(PROSIT_INDEXED_ALPHABET[i] for i in s if i != 0))\ndf['sequence_length'] = df.sequence_integer.map(lambda s: np.count_nonzero(s))"
  },
  {
    "objectID": "tutorials/fragmentation/manual-prosit-int.html#inspecting-data-distributions",
    "href": "tutorials/fragmentation/manual-prosit-int.html#inspecting-data-distributions",
    "title": "Prosit-style GRU with pre-annotated ProteomeTools data",
    "section": "Inspecting data distributions",
    "text": "Inspecting data distributions\n\ndf['precursor_charge'].hist()\n\n\ndf['collision_energy'].hist(bins=10)\n\n\ndf['sequence_length'].hist(bins=30-7)"
  },
  {
    "objectID": "tutorials/fragmentation/manual-prosit-int.html#dataset-preparation",
    "href": "tutorials/fragmentation/manual-prosit-int.html#dataset-preparation",
    "title": "Prosit-style GRU with pre-annotated ProteomeTools data",
    "section": "Dataset preparation",
    "text": "Dataset preparation\n\n# Split the data into training, validation, and test\n\nfrom random import shuffle\n\ndef split_dataframe(df, \n                    unique_column, \n                    ratio_training=0.8, \n                    ratio_validation=0.1, \n                    ratio_test=0.1):\n  \"\"\"\n  This function splits the dataframe in three splits and makes sure that values\n  of `unique_column` are unique to each of the splits. This is helpful if, for \n  example, you have non-unique sequence in `unique_column` but want to ensure \n  that a sequence value is unique to one of the splits.\n  \"\"\"\n\n  assert ratio_training + ratio_validation + ratio_test == 1\n\n  unique = list(set(df[unique_column]))\n  n_unique = len(unique)\n  shuffle(unique)\n\n  train_split = int(n_unique * ratio_training)\n  val_split = int(n_unique * (ratio_training + ratio_validation))\n\n  unique_train = unique[:train_split]\n  unique_validation = unique[train_split:val_split]\n  unique_test = unique[val_split:]\n\n  assert len(unique_train) + len(unique_validation) + len(unique_test) == n_unique\n\n  df_train = df[df[unique_column].isin(unique_train)]\n  df_validation = df[df[unique_column].isin(unique_validation)]\n  df_test = df[df[unique_column].isin(unique_test)]\n\n  assert len(df_train) + len(df_validation) + len(df_test) == len(df)\n\n  return df_train, df_validation, df_test\n\ndf_train, df_validation, df_test = split_dataframe(df, unique_column='sequence_maxquant')\n\n\n# Prepare the training data\nINPUT_COLUMNS = ('sequence_integer', 'precursor_charge_onehot', 'collision_energy_aligned_normed')\nOUTPUT_COLUMN = 'intensities_raw'\n\nx_train = [np.vstack(df_train[column]) for column in INPUT_COLUMNS]\ny_train = np.vstack(df_train[OUTPUT_COLUMN])\n\nx_validation = [np.vstack(df_validation[column]) for column in INPUT_COLUMNS]\ny_validation = np.vstack(df_validation[OUTPUT_COLUMN])\n\nx_test = [np.vstack(df_test[column]) for column in INPUT_COLUMNS]\ny_test = np.vstack(df_test[OUTPUT_COLUMN])"
  },
  {
    "objectID": "tutorials/fragmentation/manual-prosit-int.html#model-setup-and-training",
    "href": "tutorials/fragmentation/manual-prosit-int.html#model-setup-and-training",
    "title": "Prosit-style GRU with pre-annotated ProteomeTools data",
    "section": "Model setup and training",
    "text": "Model setup and training\n\n# Setup model and trainig parameters\nDIM_LATENT = 124\nDIM_EMBEDDING_IN = max(PROSIT_ALHABET.values()) + 1  # max value + zero for padding\nDIM_EMBEDDING_OUT = 32\nEPOCHS = 5\nBATCH_SIZE = 256\n\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Dense, GRU, Embedding, Multiply\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as k\n\n# Build the model with input layers for sequence, precursor charge, and collision energy\nin_sequence = Input(shape=[x_train[0].shape[1]], name=\"in_sequence\")\nin_precursor_charge = Input(shape=[x_train[1].shape[1]], name=\"in_precursor_charge\")\nin_collision_energy = Input(shape=[x_train[2].shape[1]], name=\"in_collision_energy\")\n\nx_s = Embedding(input_dim=DIM_EMBEDDING_IN, output_dim=DIM_EMBEDDING_OUT)(in_sequence)\nx_s = GRU(DIM_LATENT)(x_s)\nx_z = Dense(DIM_LATENT)(in_precursor_charge)\nx_e = Dense(DIM_LATENT)(in_collision_energy)\nx = Multiply()([x_s, x_z, x_e])\nout_intensities = Dense(y_train.shape[1])(x)\n\nmodel = Model([in_sequence, in_precursor_charge, in_collision_energy], out_intensities)\nmodel.summary()\n\n\ndef masked_spectral_distance(true, pred):\n    # This is the spectral angle implementation as used in Prosit\n    # See https://github.com/kusterlab/prosit/blob/master/prosit/losses.py#L4-L16\n    # Note, fragment ions that cannot exists (i.e. y20 for a 7mer) must have the value  -1.\n    import keras.backend as k\n\n    epsilon = k.epsilon()\n    pred_masked = ((true + 1) * pred) / (true + 1 + epsilon)\n    true_masked = ((true + 1) * true) / (true + 1 + epsilon)\n    pred_norm = k.l2_normalize(true_masked, axis=-1)\n    true_norm = k.l2_normalize(pred_masked, axis=-1)\n    product = k.sum(pred_norm * true_norm, axis=1)\n    arccos = tf.acos(product)\n    return 2 * arccos / np.pi\n\nmodel.compile(optimizer='Adam', loss=masked_spectral_distance)\nhistory = model.fit(x=x_train, y=y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_validation, y_validation))"
  },
  {
    "objectID": "tutorials/fragmentation/manual-prosit-int.html#model-evaluation",
    "href": "tutorials/fragmentation/manual-prosit-int.html#model-evaluation",
    "title": "Prosit-style GRU with pre-annotated ProteomeTools data",
    "section": "Model evaluation",
    "text": "Model evaluation\n\n# Plotting the training history \n\nimport matplotlib.pyplot as plt\n\nplt.plot(range(EPOCHS), history.history['loss'], '-', color='r', label='Training loss')\nplt.plot(range(EPOCHS), history.history['val_loss'], '--', color='r', label='Validation loss')\nplt.title(f'Training and validation loss across epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\ntest_spectral_angle = model.evaluate(x_test, y_test)\ntest_spectral_angle"
  },
  {
    "objectID": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html",
    "href": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html",
    "title": "NIST (part 2): Traditional ML: Gradient boosting",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html#feature-engineering",
    "href": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html#feature-engineering",
    "title": "NIST (part 2): Traditional ML: Gradient boosting",
    "section": "2.1 Feature engineering",
    "text": "2.1 Feature engineering\n\nimport numpy as np\nimport pandas as pd\n\n\namino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\naa_properties = {\n    \"basicity\": np.array([37,35,59,129,94,0,210,81,191,81,106,101,117,115,343,49,90,60,134,104]),\n    \"helicity\": np.array([68,23,33,29,70,58,41,73,32,73,66,38,0,40,39,44,53,71,51,55]),\n    \"hydrophobicity\": np.array([51,75,25,35,100,16,3,94,0,94,82,12,0,22,22,21,39,80,98,70]),\n    \"pI\": np.array([32,23,0,4,27,32,48,32,69,32,29,26,35,28,79,29,28,31,31,28]),\n}\n\nproperties_df = pd.DataFrame(aa_properties, index=amino_acids)\nproperties_df\n\n\n\n\n\n  \n    \n      \n      basicity\n      helicity\n      hydrophobicity\n      pI\n    \n  \n  \n    \n      A\n      37\n      68\n      51\n      32\n    \n    \n      C\n      35\n      23\n      75\n      23\n    \n    \n      D\n      59\n      33\n      25\n      0\n    \n    \n      E\n      129\n      29\n      35\n      4\n    \n    \n      F\n      94\n      70\n      100\n      27\n    \n    \n      G\n      0\n      58\n      16\n      32\n    \n    \n      H\n      210\n      41\n      3\n      48\n    \n    \n      I\n      81\n      73\n      94\n      32\n    \n    \n      K\n      191\n      32\n      0\n      69\n    \n    \n      L\n      81\n      73\n      94\n      32\n    \n    \n      M\n      106\n      66\n      82\n      29\n    \n    \n      N\n      101\n      38\n      12\n      26\n    \n    \n      P\n      117\n      0\n      0\n      35\n    \n    \n      Q\n      115\n      40\n      22\n      28\n    \n    \n      R\n      343\n      39\n      22\n      79\n    \n    \n      S\n      49\n      44\n      21\n      29\n    \n    \n      T\n      90\n      53\n      39\n      28\n    \n    \n      V\n      60\n      71\n      80\n      31\n    \n    \n      W\n      134\n      51\n      98\n      31\n    \n    \n      Y\n      104\n      55\n      70\n      28\n    \n  \n\n\n\n\n\n# Peptide input\n# Feature engineering settings\n\nproperties = np.array([\n    [37,35,59,129,94,0,210,81,191,81,106,101,117,115,343,49,90,60,134,104],  # basicity\n    [68,23,33,29,70,58,41,73,32,73,66,38,0,40,39,44,53,71,51,55],  # helicity\n    [51,75,25,35,100,16,3,94,0,94,82,12,0,22,22,21,39,80,98,70],  # hydrophobicity\n    [32,23,0,4,27,32,48,32,69,32,29,26,35,28,79,29,28,31,31,28],  # pI\n])\n\nquantiles = [0, 0.25, 0.5, 0.75, 1]\naa_indices = {aa: i for i, aa in  enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\naa_to_index = np.vectorize(lambda aa: aa_indices[aa])\n\ndef encode_peptide(sequence, charge):\n    # 4 properties * 5 quantiles * 3 ion types + 4 properties * 4 site + 2 global\n    n_features = 78\n    n_ions = len(sequence) - 1\n\n    # Encode amino acids as integers to index amino acid properties for peptide sequence\n    peptide_indexed = aa_to_index(np.array(list(sequence)))\n    peptide_properties = properties[:, peptide_indexed]\n\n    # Empty peptide_features array\n    peptide_features = np.full((n_ions, n_features), np.nan)\n\n    for b_ion_number in range(1, n_ions + 1):\n        # Calculate quantiles of features across peptide, b-ion, and y-ion\n        peptide_quantiles = np.hstack(\n            np.quantile(peptide_properties, quantiles, axis=1).transpose()\n        )\n        b_ion_quantiles = np.hstack(\n            np.quantile(peptide_properties[:,:b_ion_number], quantiles, axis=1).transpose()\n        )\n        y_ion_quantiles = np.hstack(\n            np.quantile(peptide_properties[:,b_ion_number:], quantiles, axis=1).transpose()\n        )\n\n        # Properties on specific sites: nterm, frag-1, frag+1, cterm\n        specific_site_indexes = np.array([0, b_ion_number - 1, b_ion_number, -1])\n        specific_site_properties = np.hstack(peptide_properties[:, specific_site_indexes].transpose())\n\n        # Global features: Length and charge\n        global_features = np.array([len(sequence), int(charge)])\n\n        # Assign to peptide_features array\n        peptide_features[b_ion_number - 1, 0:20] = peptide_quantiles\n        peptide_features[b_ion_number - 1, 20:40] = b_ion_quantiles\n        peptide_features[b_ion_number - 1, 40:60] = y_ion_quantiles\n        peptide_features[b_ion_number - 1, 60:76] = specific_site_properties\n        peptide_features[b_ion_number - 1, 76:78] = global_features\n\n    return peptide_features\n\n\ndef generate_feature_names():\n    feature_names = []\n    for level in [\"peptide\", \"b\", \"y\"]:\n        for aa_property in [\"basicity\", \"helicity\", \"hydrophobicity\", \"pi\"]:\n            for quantile in [\"min\", \"q1\", \"q2\", \"q3\", \"max\"]:\n                feature_names.append(\"_\".join([level, aa_property, quantile]))\n    for site in [\"nterm\", \"fragmin1\", \"fragplus1\", \"cterm\"]:\n        for aa_property in [\"basicity\", \"helicity\", \"hydrophobicity\", \"pi\"]:\n            feature_names.append(\"_\".join([site, aa_property]))\n        \n    feature_names.extend([\"length\", \"charge\"])\n    return feature_names\n\nLet‚Äôs test it with a single peptide:\n\npeptide_features = pd.DataFrame(encode_peptide(\"RALFGARIELS\", 2), columns=generate_feature_names())\npeptide_features\n\n\n\n\n\n  \n    \n      \n      peptide_basicity_min\n      peptide_basicity_q1\n      peptide_basicity_q2\n      peptide_basicity_q3\n      peptide_basicity_max\n      peptide_helicity_min\n      peptide_helicity_q1\n      peptide_helicity_q2\n      peptide_helicity_q3\n      peptide_helicity_max\n      ...\n      fragplus1_basicity\n      fragplus1_helicity\n      fragplus1_hydrophobicity\n      fragplus1_pi\n      cterm_basicity\n      cterm_helicity\n      cterm_hydrophobicity\n      cterm_pi\n      length\n      charge\n    \n  \n  \n    \n      0\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      37.0\n      68.0\n      51.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      1\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      81.0\n      73.0\n      94.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      2\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      94.0\n      70.0\n      100.0\n      27.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      3\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      0.0\n      58.0\n      16.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      4\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      37.0\n      68.0\n      51.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      5\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      343.0\n      39.0\n      22.0\n      79.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      6\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      81.0\n      73.0\n      94.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      7\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      129.0\n      29.0\n      35.0\n      4.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      8\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      81.0\n      73.0\n      94.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      9\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      49.0\n      44.0\n      21.0\n      29.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n  \n\n10 rows √ó 78 columns"
  },
  {
    "objectID": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html#getting-the-target-intensities",
    "href": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html#getting-the-target-intensities",
    "title": "NIST (part 2): Traditional ML: Gradient boosting",
    "section": "2.2 Getting the target intensities",
    "text": "2.2 Getting the target intensities\n\npeptide_targets =  pd.DataFrame({\n    \"b_target\": spectrum[\"parsed_intensity\"][\"b\"],\n    \"y_target\": spectrum[\"parsed_intensity\"][\"y\"],\n})\npeptide_targets\n\n\n\n\n\n  \n    \n      \n      b_target\n      y_target\n    \n  \n  \n    \n      0\n      0.000000\n      0.169962\n    \n    \n      1\n      0.088663\n      0.120565\n    \n    \n      2\n      0.000000\n      0.169803\n    \n    \n      3\n      0.000000\n      0.203977\n    \n    \n      4\n      0.000000\n      0.180938\n    \n    \n      5\n      0.000000\n      0.095090\n    \n    \n      6\n      0.000000\n      0.000000\n    \n  \n\n\n\n\n\npeptide_targets =  pd.DataFrame({\n    \"b_target\": spectrum[\"parsed_intensity\"][\"b\"],\n    \"y_target\": spectrum[\"parsed_intensity\"][\"y\"][::-1],\n})\npeptide_targets\n\n\n\n\n\n  \n    \n      \n      b_target\n      y_target\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n    \n    \n      1\n      0.088663\n      0.095090\n    \n    \n      2\n      0.000000\n      0.180938\n    \n    \n      3\n      0.000000\n      0.203977\n    \n    \n      4\n      0.000000\n      0.169803\n    \n    \n      5\n      0.000000\n      0.120565\n    \n    \n      6\n      0.000000\n      0.169962\n    \n  \n\n\n\n\n\nfeatures = encode_peptide(spectrum[\"sequence\"], spectrum[\"charge\"])\ntargets = np.stack([spectrum[\"parsed_intensity\"][\"b\"], spectrum[\"parsed_intensity\"][\"y\"][::-1]], axis=1)\nspectrum_id = np.full(shape=(targets.shape[0], 1), fill_value=1, dtype=np.uint32)  # Repeat id for all ions\n\n\npd.DataFrame(np.hstack([spectrum_id, features, targets]), columns=[\"spectrum_id\"] + generate_feature_names() + [\"b_target\",  \"y_target\"])\n\n\n\n\n\n  \n    \n      \n      spectrum_id\n      peptide_basicity_min\n      peptide_basicity_q1\n      peptide_basicity_q2\n      peptide_basicity_q3\n      peptide_basicity_max\n      peptide_helicity_min\n      peptide_helicity_q1\n      peptide_helicity_q2\n      peptide_helicity_q3\n      ...\n      fragplus1_hydrophobicity\n      fragplus1_pi\n      cterm_basicity\n      cterm_helicity\n      cterm_hydrophobicity\n      cterm_pi\n      length\n      charge\n      b_target\n      y_target\n    \n  \n  \n    \n      0\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      70.0\n      28.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.000000\n    \n    \n      1\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      70.0\n      28.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.088663\n      0.095090\n    \n    \n      2\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      70.0\n      28.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.180938\n    \n    \n      3\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      98.0\n      31.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.203977\n    \n    \n      4\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      3.0\n      48.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.169803\n    \n    \n      5\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      94.0\n      32.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.120565\n    \n    \n      6\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      22.0\n      79.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.169962\n    \n  \n\n7 rows √ó 81 columns\n\n\n\nNote the [::-1] after spectrum[\"parsed_intensity\"][\"y\"]. Remember why we do this?\nLet‚Äôs get a full feature/target table for all spectra in our dataset. Note that this might take some time, sometimes up to 30 minutes. To skip this step, simple download the file with pre-encoded features and targets, and load in two cells below.\n\ntables = []\nfor i, spectrum in progress.track(enumerate(spectrum_list)):\n    features = encode_peptide(spectrum[\"sequence\"], spectrum[\"charge\"])\n    targets = np.stack([spectrum[\"parsed_intensity\"][\"b\"], spectrum[\"parsed_intensity\"][\"y\"][::-1]], axis=1)\n    spectrum_id = np.full(shape=(targets.shape[0], i), fill_value=1, dtype=np.uint32)  # Repeat id for all ions\n    table = np.hstack([spectrum_id, features, targets])\n    tables.append(table)\n\nfull_table = np.vstack(tables)\n\nspectra_encoded = pd.DataFrame(full_table, columns=[\"spectrum_id\"] + generate_feature_names() + [\"b_target\",  \"y_target\"])\nspectra_encoded.to_feather(\"human_hcd_tryp_best_spectra_encoded.feather\")\n\nc:\\Users\\ralfg\\miniconda3\\envs\\proteomicsml\\lib\\site-packages\\rich\\live.py:229: UserWarning: install \"ipywidgets\" \nfor Jupyter support\n  warnings.warn('install \"ipywidgets\" for Jupyter support')\n\n\n\n\n\n\n\n\n\n\n# Uncomment this step to load in pre-encoded features from a file:\n# spectra_encoded = pd.read_feather(\"human_hcd_tryp_best_spectra_encoded.feather\")\n\n\nspectra_encoded\n\n\n\n\n\n  \n    \n      \n      spectrum_id\n      peptide_basicity_min\n      peptide_basicity_q1\n      peptide_basicity_q2\n      peptide_basicity_q3\n      peptide_basicity_max\n      peptide_helicity_min\n      peptide_helicity_q1\n      peptide_helicity_q2\n      peptide_helicity_q3\n      ...\n      fragplus1_hydrophobicity\n      fragplus1_pi\n      cterm_basicity\n      cterm_helicity\n      cterm_hydrophobicity\n      cterm_pi\n      length\n      charge\n      b_target\n      y_target\n    \n  \n  \n    \n      0\n      1.0\n      0.0\n      37.0\n      37.0\n      37.0\n      191.0\n      32.0\n      68.0\n      68.0\n      68.0\n      ...\n      51.0\n      32.0\n      191.0\n      32.0\n      0.0\n      69.0\n      22.0\n      2.0\n      0.000000\n      0.000000\n    \n    \n      1\n      1.0\n      0.0\n      37.0\n      37.0\n      37.0\n      191.0\n      32.0\n      68.0\n      68.0\n      68.0\n      ...\n      51.0\n      32.0\n      191.0\n      32.0\n      0.0\n      69.0\n      22.0\n      2.0\n      0.094060\n      0.000000\n    \n    \n      2\n      1.0\n      0.0\n      37.0\n      37.0\n      37.0\n      191.0\n      32.0\n      68.0\n      68.0\n      68.0\n      ...\n      51.0\n      32.0\n      191.0\n      32.0\n      0.0\n      69.0\n      22.0\n      2.0\n      0.180642\n      0.000000\n    \n    \n      3\n      1.0\n      0.0\n      37.0\n      37.0\n      37.0\n      191.0\n      32.0\n      68.0\n      68.0\n      68.0\n      ...\n      51.0\n      32.0\n      191.0\n      32.0\n      0.0\n      69.0\n      22.0\n      2.0\n      0.204203\n      0.050476\n    \n    \n      4\n      1.0\n      0.0\n      37.0\n      37.0\n      37.0\n      191.0\n      32.0\n      68.0\n      68.0\n      68.0\n      ...\n      51.0\n      32.0\n      191.0\n      32.0\n      0.0\n      69.0\n      22.0\n      2.0\n      0.233472\n      0.094835\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5792923\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      70.0\n      28.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.180938\n    \n    \n      5792924\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      98.0\n      31.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.203977\n    \n    \n      5792925\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      3.0\n      48.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.169803\n    \n    \n      5792926\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      94.0\n      32.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.120565\n    \n    \n      5792927\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      22.0\n      79.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.169962\n    \n  \n\n5792928 rows √ó 81 columns\n\n\n\nThis is the data we will use for training. Note that each spectrum comprises of multiple lines: One line per b/y-ion couple. The only thing left to do is to split the data into train, validation, and test sets, according to the peptide-level split we made earlier.\n\nspectra_encoded_trainval = spectra_encoded[spectra_encoded.index.isin(train_val_spectra.index)]\nspectra_encoded_test = spectra_encoded[spectra_encoded.index.isin(test_spectra.index)]"
  },
  {
    "objectID": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html#hyperparameter-optimization-and-model-selection",
    "href": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html#hyperparameter-optimization-and-model-selection",
    "title": "NIST (part 2): Traditional ML: Gradient boosting",
    "section": "2.3 Hyperparameter optimization and model selection",
    "text": "2.3 Hyperparameter optimization and model selection\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\nreg =  GradientBoostingRegressor()\n\nX_train = spectra_encoded_trainval.drop(columns=[\"spectrum_id\", \"b_target\",  \"y_target\"])\ny_train = spectra_encoded_trainval[\"b_target\"]\nX_test = spectra_encoded_test.drop(columns=[\"spectrum_id\", \"b_target\",  \"y_target\"])\ny_test = spectra_encoded_test[\"b_target\"]\n\nreg.fit(X_test, y_test)\n\nGradientBoostingRegressor()\n\n\n\ny_test_pred = reg.predict(X_test)\n\n\nnp.corrcoef(y_test, y_test_pred)[0][1]\n\n0.7504125450838988\n\n\nLet‚Äôs see if we can do better by optimizing some hyperparameters!\n\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n\n\ndef objective(n_estimators):\n    # Define algorithm\n    reg =  GradientBoostingRegressor(n_estimators=n_estimators)\n\n    # Fit model\n    reg.fit(X_test, y_test)\n\n    # Test model\n    y_test_pred = reg.predict(X_test)\n    correlation = np.corrcoef(y_test, y_test_pred)[0][1]\n    \n    return {'loss': -correlation, 'status': STATUS_OK}\n    \n\n\nbest_params = fmin(\n  fn=objective,\n  space=hp.randint('n_estimators', 10, 1000),\n  algo=tpe.suggest,\n  max_evals=10,\n)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [08:50<00:00, 53.01s/trial, best loss: -0.8465849146319573]\n\n\n\nbest_params\n\n{'n_estimators': 874}\n\n\nSuccess! Initially, the default value of 100 estimators was used. According to this hyperopt run, using 874 estimators results in a more performant model."
  },
  {
    "objectID": "tutorials/fragmentation/nist-3-deep-learning-lstm.html",
    "href": "tutorials/fragmentation/nist-3-deep-learning-lstm.html",
    "title": "NIST (part 3): Deep learning: BiLSTM",
    "section": "",
    "text": "[TODO]\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publication",
    "section": "",
    "text": "[TODO]\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  }
]